Dimensionality reduction:
	Unsupervized learning / clustring technique, tries to reduce the number of features while losing as little information as possible.
	One simple technique for doing dimensionality reduction is principal componenet analysis (PCA)
		when we run PCA on data it can construct the n most infromative features. (these features are diffrent from existing data)


in Dicision Tree:
	restring the depth of the tree indirectly restricts the number of leaf nodes a tree can have. for example, having a maximum depth of three would allow at most 8 leaf nodes for that tree.

	overfitting can also be avoided by pruning decision trees. pruning can be done while building the trees, and this is called pre-pruning. Pruning can also be done after the decision trees have been built. this is called post-pruning. Pruning usually uses the cross-validation of validation scores to idendify which portions of the trees can be pruned. Pre pruning is also known as early stopping. and it's a faster method, but there are chances a could underfit the data as it gives just an estimate of when it should stop. post-pruning methods are not based on estimates, but rather actual values, and thedecision tree is pruned once it's completely built. 

	advantages of dicision tree:
		1- it is easiear to interpret then other models.
		2- can handle multi class classification
	diadvantage of dicition tree:
		they tend to be unstable. any little variation in the training data might result in totally diferent dicition tree. thus we use random forest. 

in Random forest:
	we train multiple dicision trees, eash with diffrest variables, (and bootstraping with replacement), then predict the most prediction. (eg: 50 trees, X ==> trees, 40 trees predict A, 5 predicts B, and 5 predicts C, we choose A as a prediction since it is most predicted)

	in the prediction phase, the observation goes into all trees, and most commont prediction choosen as prediciton for thtat observation.

	disadvantages:
		more dificult to interpret the Random forest then dicision tree. 

KNN:
	widely applied in many domains
	
	it is ofter useful to use it as a baseline test, no matter what your application is. 

	we need numerical representation for each variable (to campare distance, hence choose K closest)
	similaritly between two feature vectors can be measured by distance function.

	we don't need too larger K because that means listening to very distant neighbors. and lumping things together that are actually quite dissimilar. on the other hand, we don't want too smaller K, because then random meaningless local variations can have too much influence on our choice.

	(we know nearest neighbourhow graphically, but) computers can't glance and guess the way humans can, even in the two-dimensional case. (so we use distance functions to know distance. and distance functions require a numeric vectors)

	unlike most other classification methods, KNN is a type of lazy learning, Meaning that, there is no seperate training phase before classification. This does highlight one big downside ot thir algorithm, we have to keep the entire training data in memory. and as we get more examples it gets more and more expensive to computer which neighbors are nearest. for this reason, KNN usually works best on smaller data sets that don't have too many features. 

	There are two important decisions that must be made before building KNN model:
		1- value of K
			You can choose K somewhat arbitrarily, or could try it cross-validation to compe up with an optimal value for K.

			K should also be chosen such that there will be a clear majarity between classes.

			The choice of K is essential in building the KNN.
			in fact, k can be regarded as one of the most important factors influencing the KNN classification performance.

			K is smoothing parameter: 
				for any given problem a small value of K will lead to large variance in predictions. alternatively, setting K to a large vlaue, means you don't get a very customized answer, if k is the entire set of examples, you're going to always guess the majority class.

				K should be set to a value large enough to minimize the probability of misclassification, and small enough that it's actually a meaningful subset of the total number of examples.


		2- distance measure:

			(in KNN) do wee need always calculating all the  distances? NO, 
			(we have to scale our data before calculating the distance)
			a distance function maps two elements to some real number , the distance between them.

			There are 3 rules that must bold for valid distance measures. 
				
				1- only positive numbers
					distance(a,b) >= 0 # if a = b then distance(a,b) = 0
				2- order doesn't metter
					distance(a,b) = distance (b,a)
				3- the triangle inequality must hold (detouring from the staright line can only increase the distance)

			Q: There are many differny valid distance measures, what's the best? 
			Ans: as usual it depends on your problem

			Euclidean distance.

				Pythagorean theorem:
					sqrt(
						(x2- x1)^2 + (y2 - y1)^2
						)


					E(A,B):
						SUM = 0
						for j in range(1, m+1) # in python we start from 0
							SUM += (X(A,j) - X(B,J))^2
						sqrt(SUM)

				in Linear algebra:
					finding the magnitude of the diffrence vector between two points.
				
				it is genrelize to M dimensional space.


				it is also called (L2 NORM), L2 because we're squaring the values and norms is the technical term for the magnitude of a vector.

				Pehaps the most commonly used distance for machine learning algorithms.

				it's vary useful when our features are cotinuous, but there are some situations where Euclidean distance isn't quite right. As an example, if you're a taxi driver driving through the organized street blocks of a city then the straight line distance would
				force you to pass through buildings to get to your destination. So it's more accurate to consider what path you can take along the city blocks rather than straight line. In fact, we call it Manhattan distance.

			Manhattan distance:
				if we need to calculate the distance between two data points in a grid-like space. That grid-like space can be represented by what we call a Cartesian coordinate system. Cartesian coordinates describe locations by their distance along different axes. 

				For Manhattan distance, we take the differences along each of the axes and sum up those differences. This gives us the grid walking distances between two points, formally it's the sum of their absolute differences of their Cartesian coordinates. 

				distanceManhattan(A,B):
					SUM = 0
					for j in range(1,m+1) # in python we start from 0
						SUM += abs(  X(A,j) - X(B,j)  )
				Manhattan distance is better when we're dealing with high dimensional data or when you know, you can't go straight from point a to point b, then you might as well use the more appropriate measure. And because we're not squaring the difference like we did for Euclidean distance, we're not magnifying distances. This helps deal with outliers. 

				is also called L1 distance or L1 norm

				Euclidean and Manhattan distance measures are standard and useful measures, but they're only valid for continuous variables. If you have categorical or non continuous variables, the Hamming distance might be most appropriate. 

			Hamming distance only cares about differences, when the value's the same the distance is 0, if the values are at all different the distance is 1. It's easy to see that this works even for categorical features. The Hamming distance between two feature vectors is exactly the number of features that are different.

				distanceHamming(A,B):
					SUM = 0
					for j in range(1, m+1): # since in python we start counting from 0
						SUM += abs(  X(A,j) - X(B,j)  )

				if X(A,j) = X(B,j) ==> distanceHamming<j> = 0
				if X(A,j) != X(B,j) ==> distanceHamming<j> = 1



	with the power and simplicity, KNN also comes with downside of requiring mem ory for the training dataset and a fair amount of computation.



LOGISTIC REGRESSION:
	
	two steps:
		1- first linear regression
		2- logistic transfer function take linear function output and give some number(0,1)

	predict the probabilities that a data point belongs to a particuler class.

	it called <Logistic regression> because it's doing classification using a regression operation and a transfer function.
		transfer function:
			a function whose job it is to translate the output of one function into some other space. in this case the the trainsfer functions (which is logistic function here) takes the number reported by our regression model and transfer  it into a class label.
	the value it finds via the regression step can be thought of as the probability that a data poing belongs to a particuler class.

	The threshold can be adjusted based on the domin and whaterver bias you need for each class

	The logistic transfer function (or simply the logistic function) maps the entire real line and transform it into space 0 and 1.

	if we use square loss*** to evaluate the parameters of our logistic function(as in linear regression) that is not convex, since it is not convex we can't guarantee that we can get the botom of the hill it is actually the lowest possible point. we could get stuck in local minimum. so we get complementry loss, so that you DO get a convex loss landscape, these are called matching losses, if you have a maching loss for your particuler transfer function then the minimum point IS the global minimum which means you CAN find the best line. so once you solve the linear regression problem you can use the solution point and it will be the best solution after applying the logistic transfer as well. 
	logistic loss:
		squared error loss of the sigmoid function if the label is 0:
			y = -log( 1 / (1+e-f(x)))
		squared error loss of the sigmoid function if the label is 1
			y = -log( 1 - (1 / (1+e-f(x))))
	if you use this matching loss with a logistic transfer function you get a convex and smooth landscape. Now gradient decrease find your optimal model. 
	***	squared error loss of the sigmoid function if the label is 0:
			y = ( 1 / ( 1 + e-f(x))^2 )
		squared error loss of the sigmoid function if then label is 1:
			y = ( 1 - ( 1 / ( 1 + e-f(x))^2 ))
in Linear regression:

	Non Linear features and model complexity:
		
		to overcome underfitting we need to increase the complexity of the model. 

		Luckly, we can get complexity in Linear function through features enginearing.
		 	polynomial, square root, log
		
		polynomial:
		 	multiplies things together.
		 	the order(degree) of polynomial tell you how many times you multiply then.
		 	order2 polynomial = squared = X * X
		
		we create non Linear functions by adding or changing features that are non Linear functions of the orignal features.

		non Linear features does increase the complexity of the model which has sight affects. thre are some issuse we have thing about when we use non Linear features to add complexity to the model. 
			(much hight degree of polynomial feature causes overfitting, so be carefull)








	Linear functions have handy features like convexity and differentiability.  
	Goal: 
		find parameters (weights) that minimize the loss function.

	least squares AKA L2 loss function.

		this loss is useful because it is gives a shape that have a unique point for the minimum panalty(SSE).
		smooth and convex. which means it is going to have unique minimum point, we don't know what that minimum is, but using calcules we can find id.

		Q: is this always the panalty you want to use?
		A: NO.

		Q: we could just take the absolute value of diffrence (y^i - yi) to get the actual magnitude, why not do that?
		A: we do, and that is totally walid loss function, is is called <L1 loss> or <least absolute error. when we use gradient descent bot L1 and L2 are convex, but L1 has sharp corver <V shape> at minimum value (thus this is not smooth, thus it is not differentiable, we can't take the darivite of the L1 loss to find which parameter gives us the minimum panalty like we did with L2 loss function.> instead of smmoth curve like L2 <U shape>. 


	gradient descent:
		very commonly use in machine learning

		in gradient descent we rely on the property that our graph (loss vs slop) is smooth and continuous, so that we can take tha derivative, and convex so thre is no more than one minimum value.

		an iterative method, we start fron any point then make small adjustments in our perameters(in regression the coefficient/s) to bring us closer to an approximate solution. 
		we choose point where darivite is 0.
		require smooth function (so we don't use L1 loss function)

		We randomly pick a starting position and imagine placing a ball there. For minimizing loss, this amounts to assigning random parameters to our model. This is simply an initial guess, we have to start somewhere before we can improve it. Say our random initialization is peer. We hinted that the ball will roll down the pill somehow, but we can exactly calculate in what direction the ball will roll using the derivative or the slope of the tangent to the graph at this point. We'll use this derivative to take small steps towards the minimum of our graph. We have this mathematical notation for how to take a step or in other words, how we can tweak that parameter w. We update it by taking the current estimate then subtracting alpha times the derivative with respect to w of the loss function of the current estimate. Let's go through that bit by bit to see what's going on. Of course, we start at time zero, so w0 is our first guess for our model parameter. Now we want the ball to roll down the hill, but we need to know which direction is down. Hurray for calculus, that's the gradient of the loss function, the derivative with respect to w. So that's what direction to step, but how far do we adjust w? We need to know how large a step to take. There's a parameter for this known as the learning rate or step size. Here in our equation, we've represented this with the Greek letter alpha. It's your choice what learning rate to use, but it's some number greater than zero or you wouldn't learn anything and smaller than one or you'll over correct. Multiplying the learning rate by the gradient determines how much we should change the current position on the horizontal axis. Remember, this is the value for our models parameter w. Notice that the steeper the gradient, the bigger the change and vice versa. When the gradient is less steep, the learning rate multiplied by the gradient will be smaller. So the amount we update our position by will be smaller. This is true even if we have a rather large learning rate. What is a good learning rate to use then? It turns out this is a hard question to answer. A good learning rate is one that's not too big because you'll end up stepping past local minima and you never settle on a good parameter value. But can't be too small or it will take forever to converge, to settle down near the best value. So we want to have alpha that's as big as possible so that it will learn quickly but not one that's too big and diverges. Some techniques have decreasing alphas so that you can start big but settled right down, and depending on the rate of decrease, you can actually guarantee convergence. But it's usually simplest to sacrifice optimal convergence for our convenience and use some small alpha. The best alpha depends entirely on your particular problem. When in doubt, try 0.1, if that seems unstable, try 0.01 or start with a default set by your particular library. Enough about alpha. Let's look at the key operation in this equation, subtraction. We're taking our current position and subtracting the learning rate multiplied by the gradient. This gives us our new value which we really want to be closer to the optimal value. So why subtraction? Notice that at time t, we're here on our graph. In our 2D example, we can see that in order to get closer to the minimum, we want to step to the right. Stepping to the right is equivalent to increasing w along the horizontal axis. Notice that at this position the gradient is negative. The learning rate is always a positive number so subtracting a negative number from our current position moves us to the right. Similarly, if we'd been located here at time t, we'd want to move left to get closer to the minimum. Our gradient at this point is positive and so subtracting it from our current position will make w smaller moving us to the left. To summarize, because we've made sure our loss graph is smooth and convex the derivative or gradient always exists and always points away from the minimum point. In order to get closer to the minimum, we move against the gradient. This is why we have a minus sign in our update equation and also why we call it gradient descent. The process is iterative, so after subtracting the gradient multiplied by our learning rate, we have a new position for our ball. We calculate the gradient at this new point, multiply it by our learning rate and subtract that from the current position. Repeat this process over and over until we're not changing our position very much because the gradient has gotten so small. This lets us converge on or close to the minima. What does convergence mean? Remember, that local minimums happen when the derivative is equal to zero. As we step closer and closer to the minimum, the derivative will get closer and closer to zero as long as we're not overstepping. When we arrive at the minimum, the change will be zero and so our parameter value won't change anymore. We converge to the position where the minimum occurs.


	Across all loss functions, there's one crucial property that each must
	possess if you want to guarantee you're finding the global minimum. This
	property is convexity. The function is called convex if, for any two points
	on the graph of the function, the line segment connecting those two points
	lies above or on the graph. If you think of functions as describing some
	surface, you have to be able to attach the ends of a string anywhere along
	the surface and pull that string flat. If there's some way to attach the
	string where you can't make it absolutely flat without intersecting the
	surface, then it's not a convex surface. Why is convexity important?
	Convexity guarantees that our function has no more than one local minimum
	value.

		When our graph is not convex, we don't know for sure we've found the absolute smallest minimum value unless we explore the entire space. Whereas, no matter where we drop that ball on a convex graph, we'll reach the same final point at the only local minimum. That's the key, convex functions have at most one minimum value. So regardless of whether or not you use derivatives, if your function is differentiable or some other iterative numerical solver, if your function is not differentiable, when your function is convex, you're guaranteed to find the optimal solution in that hypothesis space. So one local minimum means we're able to find the smallest penalty value for our loss function. And if our loss function is differentiable as well as convex, this means we may be able to find an optimal function by setting the derivative of our loss function to zero, as we did in the case of L2 loss. 


bias variance tradeoff:
	
	complexity is not always good. we need our model to be complex enough to capture the relevent patterns in the data but too complex menas heigh variance and it is not ganralized well. 

	as we complex our model; in some range of complexity the bias and variance both decrease, after that poing the bias still decrease and variance increase (means: we begin overfitting), we have to choose optimal model.

	(very low bias: overfitting)

	one way to ovoid overfitting is to refuse to allow complex models.

	if we completely remove bias by adding complexity in the model, we have no bias but we are overfitting, means: the variance is too heigh. as we simple our model the bias is height, but variance is low.

	more complex the model: the low bias and heigh variance.

	algorithms with high bias will underfit the trainig data as well as perform poorly on the test/validation data.

	variance in the model is a bit hard to measure. you can look at how much you output cahnge is when you repeat the learning process under differen sample of the training data. you can also take advantage of relationship between complexity and variance, and use tricks incurage simple models.

	Reguralization A systamatic way of macking of tradeoff between bias and variance.

	learning curves can also be used dignose your model performance. 



Reguralization:
 
	A systamatic way of macking of tradeoff between bias and variance.
	A way to avoid too complexity in the model.
	
	when a learning algorithm has an objective function that only penalizes mistakes as with the loss functions we've already described, there's no reason for it to choose a simple model. So as usual, we need a mathematically precise way to measure simplicity if we're going to ask the computer to consider it. We want to modify our objective function with some criteria that rewards simplicity as well as accuracy. Just like we converted best line to least bad by penalizing mistakes, we can convert most simple to least complex and add a penalty for complexity. This is the idea behind regularizers. A regularizer is an extra mathematical term added to the objective function which penalizes complexity. Now the objective you want the learning algorithm to minimize is a combination of the penalty for making mistakes as well as the penalty for using a complex model.

	As your model gets more and more complex and more and more fitted to the training data by using higher and higher degree polynomial features, you can observe that the weights are getting larger and larger. The magnitude of weights looks like a good proxy to measure complexity and that's often what we use. We use a regularization term to penalize extreme values for our parameters. How do we measure extreme values? We sum them up. As usual, since we care about the magnitude and would prefer to keep things differentiable, the L2 norm is our friend. Remember that the L2 norm squared is the sum of the components squared. It's the magnitude of the weight vector. Remember also that the L2 norm penalty makes extreme values cause even more lost than an L1 norm penalty would because it's squares those values. But we want a little more control over this term because we might want to explore how much we should penalize the weight values. We do that by turning to another common trick in machine learning, introduce a coefficient, in this case lambda. Lambda acts as an adjustment knob to control the relative strength of carrying about simplicity compared to carrying about loss. With lambda equals zero, your objective function simply considers loss like before.

	As you turn this knob up to increase lambda, you penalize the learning algorithm more and more for complexity via the size of the weights. This means, your higher degree model gets simpler and simpler through having less and less extreme values which has a slight cost in terms of increasing average loss on the training data, but there is a benefit. You reduce the variance in your model generalization. This usually improves performance on test data at least up to a certain point. Lambda is another one of those smoothing parameters. It lets you control the trade-off between complexity and generalization between variance and bias in your model creation. However, this is not the only thing we get with regularizers. In some cases you can get interesting benefits by using different kinds of regularizers. For example, using an L1 norm of the weight vector as your regularizer term causes many models to try to find a sparse solution. This means the algorithm will try to find a model whereas many of the parameters as possible are equal to exactly zero instead of just being small. This means you can discard many of the features and use a smaller subset of features in your model.

	Regularizers also provide other benefits when combined with different kinds of specific losses. For example, when you combine L2 regularizers with SVM classifiers, we get the ability to use kernels. Kernels let you classify data points in a complex and sometimes infinite-dimensional space of nonlinear features.

	The concept of regularization can be applied almost anywhere

	Linear regression with an L2 regularizer is called ridge regression.
	Linear regression with an L1 regularizer is also called Lasso regression.



 Neural network:
 	neuron is basic component of Neural network.
 	each neuron take some numbers of input and give a (single) number as output.
 	typically each neuron actualy envolve two saparate simple functions. 
 		1- a linear function.
 			because linear functions are differentiable and convex, it realy easy to calculate, out hypothesis space is too small when we only look at linear function. 
 		2- then a non linear transfermaton(AKA activation function).

 		multiplies its weights against the input values and returns the sum. then that value is passed to activation function, and outcome is some other number, this output is the final output from this neuron.

 	Neural network learning these weights using gradient decrease.

 	output of on layer is a input of next layer.
 	input layer , following by hidden layer/s , and the output layer(which is actualy a predictions)
 	in between the input layer and output layer can be ANY number of hidden layers. 
 	the layers can be loop back from themselves, but dafault is the feed forward Neural network; that feeds each hidden layer with the output of the layer before.
 	one we have multiplied hidden layers the Neural network is called deep Neural network. and the process of learning is Deep learning. 

 	sigmoid function = logistic regression
 		take inputs from privious layer, run linear regression, the output of linear regression go into logistic function, and give some output(0,1)

 	with then Neural network we don't need to optimize each neuron saparately, 	it isn't clear how we could optimize each neuron saparately. we know what we want the entire network to do. but we DO know we need each layer to output values that ware makes the work of next layer easier. 

 	Neural nets can't be used for everything, we can't explain the results. 

 	the huge hypothesis space explored by neuron networks created variance, this is why the lots of data is incredibly important.


SVM (suport vecter machine):

	the combenation of the loss and the transfer function has to be a convex surgate. 
	lossMisclassification(y^i, yi) =  1 if yi*y^i=-1, and 0 if yi*y^i=+1
	+1 * -1 = -1
	-1 * +1 = -1
	+1 * +1 = +1
	-1 * -1 = +1
	with this missclassification loss  we treat every missclassification equqly bad, but may be some missclassifications are worse then others. remember we are trying to decide how bad a linear saperation of the two classes is. may be we can impose stronger panalties for samples found deeper in taratory of the other lass. in fact we wouldn't mind the margen for errors. so even correctly classified samples  but are really close to the line almost missclassified could increase the panalty. 

	hinge loss function:

		this function solves some of the problem created by other loss functions, and can be used to turn the power of regression towards classification. 
	
		we compute the panalty:
			z^i = xiT w
			and then use sign function to determine which side of the line taht particular example falls on.
			y^i = sign(z^i)		

			if z^1 = 0: 
				then that example x is exactly on the decition boundry.

			so in the linear part of our function the points further from the line will be mapped to increasingly positive or increasingly nagative numbers, depending on which side of the line they are on.  we can use that number (z^) as a notion for signed distance from the line. we can use it to create a panalty that magnified by distance from the linear sapareter, if we take this notion of signed distance (Z^) and multiply by the correct lable (y) we get that number that posative if the sample falls in correct side of the line. or nagative if the point falls in wrong side of the line. and it is really positive a very large number if it is realy far on the correct side of the line.  and it is realy small when it is quit close to dicision boundry.	
			 

			we want to penalizes nearly wrong classification create some margen fro error arround the dicision boundry. we typically give ourselves margen of 1, we want to penalizes plasing the line less then 1 unit away from a point evan if it is correctly classified. for this we just need to subtract our y*z^ from 1 <1-y*z^>, this value will be posative for any point that encroaches on that margin, or is incorrectly classified by our line. 
				this way not only incorrectly classified points get panalty but correctly classified points that are in that margin space around the dicision line will also recive small panalties. 

				how ever if use this <1-y*z^) as our loss  the correctly classified points will get a nagative panalty which is wrong. we can fix it by filtring losses less then 0, this bring us the final formula for hinge loss:
					max{0, 1-y*z^}  

				we sum this panalty over all training data, and that is our total loss. 
				this hinge loss in convex (oposite to misclassification loss). 
				we are ready to use the hinge loss to choose good linear saparater for our classification task(SVM). 

	SVM a powerfull techniques for classification. 
	
	the best line maximizes the distance between itself and the closest data points from both classes. This distance is known as the margin. Since Support Vector Machines are trying to maximize the distance, SVMs are also called large margin classifiers, which is the line that creates as much space as possible between the closest points to the decision boundary, which are at least two, one from each class. More precisely, in the case where the data is linearly separable and we don't allow any misclassification, it's the line that's the same distance from the closest points from both classes. These points which are closest to the best line are called support vectors. Thus, the machinery that uses such a line is called the Support Vector Machine or SVM. SVMs are nothing other than a linear model with a specific separating line. That specific line is the line that causes the widest minimum margin. So what optimization process is happening while that SVM is being learned? You won't be surprised to hear that hinge loss takes the center stage. Just like linear regression finds the weights that minimize the loss and can use regularizers to control complexity, a Support Vector Machine finds the weights that minimize the aggregate hinge loss with an L2 norm as weight regularizer. However, the L2 regularizer is playing a very important role here. It's the L2 regularizer that's ensuring the minimum margins are being maximized.

	we can show with math that the minimum margin is actually two over the L2 norm squared of w

	So minimizing the L2 norm squared regularizer causes maximum minimum margin. The constant C in the equation shown is the only new piece of this puzzle. Just like before, we have an arg min over the weight vector w and b. The inner term calculates the penalty over all the mis-classified values. Here, it's shown in the hinge loss formula as the max of 0 and 1 minus y times z hat. That's multiplied by a constant c, and then the L2 norm squared of the weight vector is added. If your data is linearly separable and you use a very high value for c, minimizing the SVM objective function finds exactly the line that gives you the widest minimum margin possible. This is due to the interplay between three things. One, the triangle you see on the positive side of the yz hat axis, the fact that you're penalizing anything that has a margin less than one. Two, the fact that you can change the magnitude of the w vector without affecting the direction of the line. Three, that you're pushing w to be as small as possible with the regularizer added and if the data is not linearly separable, its penalizing misclassifications more and more as they show up deeper and deeper on the wrong side of our decision boundary. C is a smoothing parameter that controls the trade-off between model complexity and accuracy. When c goes to infinity, the SVM will be called a hard margin SVM. Other c's make soft margin SVMs. Hard margin SVMs work only on linearly separable data and will force all hinge losses to be exactly zero. In soft margin SVMs, the smaller the c used or the softer you get, the more the algorithm will prefer to put points on the wrong side of the separation line if it keeps the norm of the weight vector down, causing a bigger margin among the points that are on the correct side, but also likely increasing the generalization power, at least up to some point. So what does that make c? Insert quiz here. Optimizing the SVM objective function can actually happen several different ways, and the good news is you don't really have to worry about how it happens. It can use a gradient descent method or quadratic programming, which is an iterative method. So when are SVMs a good choice of learning algorithm? Well, because of the kernel trick which we'll discuss next, they actually handle nonlinear relationships rather well. To be fair, so do your neural networks but because SVMs are based on convex optimization, you don't need as much data to have confidence you're finding the best model within the hypothesis space. When you do have lots of data in the millions of examples, SVMs can be a little slow to train. But predicting with an SVM model is quick, especially compared to Kernels with large datasets or deep neural networks. They do require homogenous appropriately scaled features. If you have mixtures of features, such as categorical and numeric, you probably want to consider decision trees or random forests.

	you can use SVMs for regression as well

	
Regression assesment:
	MAE (mean absolute error)
	MSE (mean squared error):
		if we want to convert the units back to orignal form; we can take the square root of MSE, and this known as RMSE (root mean squared error), we might want to report RMSE rather then MSE just so that we have some intuition about what that average error actually means. but is it unnecesery to do square root operetion if you mostly looking at comperison.

		MSE is most common error measure.

		There is one side effect of squaring the errors, squaring big numbers even bigger, so while MAE penalizises each error equalantly, MSE in increasingly sensitive as the errors get bigger. in other words: by MSE the model batter of macking several small mistakes then one big one. The practical consequences of this are that MSE sacrifices perfect fit on a few data points to compensate for big mistakes. In other words, one outlier in the test data makes our model look worse under MSE than it would under MAE. Sometimes this is what you want. If you have confidence that your outliers represent actual phenomenon that needs to be modeled you should be emphasizing large mistakes. There's another factor we might want to consider in our assessment of our models. Remember how different kinds of mistakes can matter more than others depending on your problem. Maybe there's some reason that underestimating values is not so bad as overestimating them, which can be especially true when the model will be used for classification, or maybe there's a certain range that's really important to get right. In that case you want a weighted error function. A weighted error function lets you assign different weightings to different kinds of error, for example, penalize overestimating values twice as heavily as underestimating. This is more common in classification error measures. 

	mean squared log error adds a log function to the calculation of the difference. This results in underestimates being penalized more than overestimates, and more importantly, evening out the effect of mistakes, not being as sensitive to outliers. It's useful when you care more about relative mistakes than the absolute magnitude of your mistakes. 
	
	R^2:
		1 - SSE / SST 
		R^2 looks at the variation or noisiness of the model labels. In particular, it modifies the same square error measure as MSE, but divides by the noisiness in the labels themselves. This means R^2 is normalized, and it lies between 0 and 1, with 1 being the best score. R^2 is usually used for interpretation without knowing the scale of the data and it explains how well the variation in the data itself explains the variation in the predicted values.


classification assessment:
	type2 error = FN(false nagative)
	type1 error = FP(false positive)


	confusion mutrix:
		columns : actual claass
		rows    : predicted class

	Accuracy
	Precision: 
		TP / (TP + FP)
		measures how mush can trust the accuracy of your positive examples. in other words: of all the things we called positive, what percentage were correct? Precision of 1  means we never call a transaction fraudulent inappropriately. When our QuAM called a transaction fraudulent, you can be confident it's correct. 
	Recall:
		TP / (TP + FN)
		Recall measures how much you can trust the coverage of our positive labels. In other words, what percentage of all the positive instances did we catch? Recall of 1 means we didn't miss anything. 

	If you know what's most important not to call something category X when it isn't, then you should be evaluating your models using precision as well as accuracy. If you know what's most important not to miss something belonging to category X, then recall is the most important. But what if you want to balance between those two? Well, use the F-measure gives us the ability to do that.

	F-measure:
		When the F Score is 1, it means we have perfect precision and recall. That also means we have perfect accuracy. So we do already have a measure for that. The usefulness of S QuAM comes into play when we don't have perfect accuracy. It reports the average, specifically the harmonic mean, but you don't need to worry about that, of precision and recall. The default way to use F score is specifically F1 score and that puts equal weight on precision and recall. The weighted F beta score let's use smooth between precision and recall with beta less than 1 putting more weight on precision and beta greater than 1 weighting recall more heavily. There's another case where plain accuracy may not be good enough when you have imbalanced classes. Then misclassifying those rare instances counts just as much as misclassifying the other. But if there's only a few examples of the rare case, overall the significance of those mistakes is way less than the significance of the other. In the worst case, the best hypothesis will be the one that completely ignores the rare cases and just predicts the majority class. This may be fine in some circumstances, but not if the QuAM building is meant to detect those rare cases. When you want to know how well classifiers performing specific to a class label, you can use class wise accuracy. Evaluation measures are specific to the domain. So you should think about how your model's going to be used to determine what the most useful measure is. The evaluation measure you use on the test data is what you use to determine how well your model is going to actually perform in the real world.
 
		F1:
			2 * ((precision * recall) / (precision + recall))

learning curves: 
	find reight complexity for model, and avoid overfitting.

	two key	 factor effect the generalization ability of model:
		1- model complexity
			as model complexity increases; decrease training AND test errors. after cirtien point the model begin overfitting.

		2- dataset size
			if you have more data for training; the ability of your model to generalize almost always become batter.
				
			(givin the model complexityi is fixed) as increases our data size the training error increases and test error decrease.
				training error increases because the variance of the training data increased where the model complexity will fixed.

				test error decrease because the model can now generalize batter.

			If we were to plot the learning curves of the training test data for different training data set sizes for fixed model complexity; The model will have low training error when we use a small training data set. However test error will be really high because it hasn't learned much about the variations in the data distribution to be able to predict for out-of-sample instances. In other words, the data points the model hasn't seen before. However, when more training data instances are used, training error increases. This is because the variance in training data has increased while the model complexity was fixed. However, test error goes down because the model can now generalize better.Another type of learning curve that specifically measures the performance of classification models is the: 

	ROC (or Receiver Operating Characteristics curve)
		For binary classification, when you have a choice of threshold as in the case of transfer functions over probabilities, the ROC curve lets you see the trade-off between precision and recall. It illustrates the capability of the classifier to distinguish between two classes as the class discrimination threshold varies. This is achieved by plotting true positive rate against false positive rate.remembter: recall is the true positive rate and false positive rate can be considered false alarms. A point in the ROC curve represents the classification model with a specific threshold setting determining the class and the ROC curve represents the collection of such points. The best possible classification model with the right threshold setting would give a point in the upper left corner or the coordinates 01. This essentially means no false negatives and no false positives. On the other hand, a random guest such as a fair coin flip would give a point along a diagonal line which goes from the bottom left to the top right. Any point which falls above this diagonal line represents a classification better than random guessing. While the ones which fall below represent classifications worse than random guessing. AUC or Area Under the Curve of the ROC curve represents the probability a classification model will rank a randomly chosen positive instance higher than a randomly chosen negative one. In other words, get it wrong. An AUC values varies between zero and one, where one means the classifier has excellent performance in separating between two classes and zero means the worst possible. 0.5 means the model doesn't actually separate the classes. Contrasting training with test error lets you see the cost of complexity and know when decrease in training error is actually hurting you. For classification, you can use the AUC ROC values as a more nuanced tool to see the trade-off between precision and recall.
	
I.I.D:
	independent identical distributed data.

	None of the observations (spacially None of the lables) depends on each other in a probablistic sence. this whill  mean you can shuffle the order and your label for one example doesn't change the probablity of the correct label for the next.	

	Not I.I.D:
		eg: time series data.

		randomely split NOT IID data into train and test causing two problems:
			1- you just lost the ordering; which is importand here.
			2- you don't actually have a cleaned test set. 
		
		there are 2 ways to deal with this:
			1- get rid of dependencies.
				you can't do it for any odd data. it depends on the data.
			2- not split your data randomly.

splitting data into train and test:
	(when we split our data into train and test, we make sure same distribution of outcome variable in bot tran and test set. eg: we have 100 observations, and label has 2 classes, 0:80%, 1:20%, we make sure after the split that the both train["target"] and test["target"] have the  0:80%, 1:20%)  

	we NEVER use test data to improve our model. (eg: it is wrong to split data into train and test, then train your model; then test on test data; after testing you go to tune your model. this is wrong. the test data should use ONE TIME ONLY, after you all tuning using validation set.). so the right way: 
		1- split into train, validation, and test chunks. 
		2- train on train set
		3- validate on validation set
		4- tune your model
		5- repeat 2,3,4 until the model is final.
		6- test on test set. and the results SHOULD NOT be used to tune the model.
		7- deploy the model.


CV (cross validation):

	Cross validation is mainly used when the dataset is relatively small. results of cross validation will be a good approximation of the result we wol'd have gotten if have used the full data set. This can help alleviate the problem with small datasets able to represent the true distribution at least to some degree. 

	we use CV to tune for hyper	parameters in general. 

	when you have lot of data make a validation set; the same you make a test set. (but) when you don't have data to spare use the appropiate cross-validation techniques. 


hyper parameters tuning:
	finding a best hyper parameters set is called Metalearning.

	(in dicision Tree:
			max depth
			min samples split (dont split further if node have less then this vlaue observations(amir)) 
			min samples leaf  (dont split further if one of the resulting nodes have less then thin value observations(amir))
			criteria for quality of splist (gini index, entropy)
			cp
			threshold (in classification)
			num trees (random forest)
			replace   (random forest)
			variables qty (random forest)
			sample.fraction (random forest)
			minimum_improvement (if at any point; spliting the space <= this value then don't split it)
			pruning (after or before)
			MSE or MAE
			weights
		in logistic regression:
			threshold
			weights
		in linear regression:
			alpha (if Reguralizers are used)
		in SVM:
			c
			kernal (some of the kernals require their own perameters, eg: polynomial kernal require it is degree of the polynomial, gamma for non linear kernal)
			weights
			distance function
		in KNN:
			k
			distance function
			weights
		in clustring:
			K (in k-mean)
			distance function)

	paremters of model that we have to put in the model. eg: max_depth and min_node in dicision trees, and threshold in logistic regression, and alpha in regularizers, and learning rate in gradient descent, and so on...

	on the other hand <model parameters> that the model is care about, eg: weights in the Neural network nodes and the split point in the dicision tree.

	thre are couple of techniques to identify best combination of hypter parameters:

		1- Grid search 
			computationally expansive
			systematic aproach,  you are give the list os pre determined combinations. the model trained on each combination, and you pick bech of them.
		
		2- random search 
			you provide a range of values for each hyper parameter. the system select random values within these ranges, and try diffrent combinations.

			it is usually finds optimal parameters or close to the optimal parameters when the data is small and the number of features is relatively few. 

			Researches shown that the random search actually does a batter job of finding optimal parameters then grid search. 
		
			instead of list of which value explicitily to try for each hyper paramter(as in Grid search), we have two options:
				1- list
					the functions will randomly sample value without replacement from the list. 
				2- distribution  
					you have also to supply the sampling method. 


different algorithms put different constraints on your data, so you end up needing to look at what algorithms are available given the data you have and how to adjust the data given the algorithms you most want to try. Of course how much data you need depends quite a bit on what algorithm you're using. For example, decision trees can be effective with a relatively small dataset, but neural nets need a very large amount. KNN can work well with a fairly small amount of data, but you need to keep that data in memory and accessible whereas other algorithms don't need further access to the training dataset once the training phase is done.


(some possible new feachures)(amir):
	numeric:
		logs
		sqrt
		polynomial (diffrent df's)
		bins (diffrens qty of bins)
		ordinal bins (you can assing numeric values for each bin) 
		binary	(diffrens thresholds) (for one or more columns, example of two_columns binary: 0 if age > 20 & sex == "Male" else 1)
		intrection
		scaling (standrization, minmax)
	catagorical:
		merge rearly accoured into opprepreate class
		if fine ordinal, use as numerical variable
	temporal(time serires)
		moving average:
			particular statistic for some previour observarions (eg: mean of last 7 days) 
	image:
		SIFT (scale invariant feature transformation)
		describe local features in image

usefull / ununsefull features:
	we al
	0 variance means 1 unique value. so the feachures is useless.
	near 0 variance variables should be more investigated. (eg check moest frequent value ratio against 2nd most frequent value ratio)

	more variance more informative the feature.

	you can compare unique(n) / LEN(n). for features drawn from discrete set this can be MORE informative than the statistical variance.

	posetive correlation: the two variables increase or decrease together.
	natagive correlation: the one variable increase the other decrease.

	in linear regression multicolinearity makes the algorithm numericly unstable and unable to find optimal model.
	to avoid multicolinearity transforming features using PCA or performing feachure selection or just using dicision trees could be  helpfull.

	if the % of missing value in an variable is heigh it is unlikely to be usefull even with the best data imputation techniques.
	
	linear algorithms want to have correlation with the label You might be tempted to do a univariate assessment and throw away individual features that have low correlation with our Target but it's important to understand that the features are going to be used in a multivariate setting a feature might not have good univariate correlation with the label but still be useful for the model interactions between it and other features may have a significant impact on the prediction power of the final model. So it's a good idea to test the usefulness of a feature in a multivariate setting if possible in the same setting as to how the features will be used finally with the learning algorithm. But of course complex nonlinear models that can exploit multivariate correlations are not always very explainable.

How many variables?:
	as the dimantionality increases the voluom of space (where the datapoints is live) increases as well. 

	each new dimension exponentially increases the amount of space we're trying to fill with our learning data. If we don't get a corresponding increase in the number of data points, we obviously are going to have less and less coverage. In other words, as the dimensionality of our data increases our coverage gets more and more sparse, for most statistical or machine learning modeling techniques sparsity is a huge problem. The amount of data needed to have the same level of performance increases exponentially as we add future dimensions, most of the times when we look for patterns in the data we want to capture areas with high density or groups of data points having similar properties. However, with high dimensionality those points just get more spread out, this makes them look really different in that higher dimension, even if they truly are near, on, or in some subset of the space. This phenomenon is popularly referred to as the curse of dimensionality. There are some algorithms that are more affected by the curse of dimensionality than others, for instance, distance measuring algorithms, like K and N or K-mean are greatly impacted. Since adding dimensions to the data, literally, increases the distances between examples, on the other hand in random forest algorithms individual trees look at subsets of features at a time. They can ignore the vastness of empty space for better and for worse, this focus can make it easier to optimize for each tree. So they don't feel the curse of dimensionality to the same extent as distance based algorithms, but there's no getting away from the fact that more features means exponentially more space to explore.

	If you have more features than data points you're in a very particular learning regime, it's under constrained, you couldn't possibly solve for a linear model in this case. There's just not enough data to tell you what all the weights should be, but we see this scenario in bioinformatics and precision medicine, a great study may have thousands of participants, but genetic information is quite a bit more complicated than that. Graphical models, Bayesian learning methods, and dimensionality reduction are all possible ways of dealing with this

	When working on day-to-day problems, domain knowledge is one of the most important factors in finding critical features that are most likely to impact the model performance. It's one of the reasons we stress the importance of advice from domain experts on the data features, and awareness of the business process overall. By just looking at the data you can consider things like correlation between features, it's not always bad to have correlated features, but it's also not always good. Of course, it really depends on the specifics of the problem and the learning algorithm, in terms of the number of features and the degree of correlation, and, of course, curse of dimensionality.

	
if we do know (population) probability that's ganerating our data; that's our (model). we can ask a questions about the probabilities of events, and get answers directly from that model. but most of the time we don't have an absolute refrence for what the best probability distribution actually is. however; if we have enough data comming from this distribution  we can estimate it. and dependin on the data quality and valume as well as how well behave the underlying probability distribution truly is; we can use the data to train a (model) that con be reasonably accuratly predict new instances ganareted from the same distribution. 

the critical piece is that the machine learning prediction task ware trying to solve is dependent on the specific data ganareting distribution that underlies the task; that was gives us the training data; that was the learning algorithm trying to model. 

since there is no universal learner(universal learner: one algorithm that perform best in all situations / data sets) we have to try diffrent learning algorithms to identify the best one for givin task. 

make sure the selected algorithm is the best for the task at hand we need a large enough dataset to represent the data ganareting distribution of that task. if you dont have enough data to represent tha task there is the good chance we can pick the wrong learning algorithm. and not ONLY that; it has to be quality data. 


DAU (data acquisition and understanding phase):

	the DAU phase is the time when you're actually going to get into data and start investigating what you have.

	The DAU phase is where all of your prior assumptions are going to be tested. If we made a wrong assumption or have done a bad job in the BUPD phase, it will now be revealed, and of course, if we do a bad job in this phase, it will also trickle onward. So we'll be careful. We have a clearly defined problem to start working on. 

	get the data from your client, hence data acquisition. But it's not just about getting one big file, we're usually going to have multiple data sources that help tackle our problem. You'll need to address all data engineering and data platform aspects at this point. After all, the moment you get the data you have to put it somewhere. Once you've gathered the data and have decided on where to store it, you need then some way to actually query it. This needs to be set up as well. If the dataset is very simple, this is not a problem but of course, it's usually not simple. How long this will take depends on the tools and the complexity of the data itself. Another aspect to data acquisition is verifying that all the data that you and the client agreed was important, actually appears in what you received in the form you expected. Do all the features that you discussed with the business initially actually appear in the data you are given? Does the format and structure of the data match the discussions you had with the business? Now is the time for you to verify if the information you need is actually there. If there's data missing, try to understand why it's missing or if you can acquire more data to fill in the gaps, and of course, document as you go. As you're doing these verification steps, record your observations and better yet, save any code so that it can be part of your overall data quality assurance process. Now that you have the data, it's time to start making sense of it. 

	data cleaning. Data cleaning is all about alignment, normalization and removal of bad values. 

	So to start making sense of your data, you'll very likely have to align different datasets or at least multiple tables. When a business builds a database system, they usually normalize the databases so that they have unique small sub tables that are easier to program and store. But for data analysis and data science, you want to denormalize these tables to aggregate and create one big table where all the data is in one place for your queries. In addition to aligning multiple tables, part of your data cleaning will also include filtering out certain values. Errors need to be filtered out and null values need to be removed. During this cleaning stage, you want to get a feel for what's going on in the data with the help of the domain expert.

	This is where you convert data into an appropriate form for modeling.

	It's important to start with identifiable steps and reusable code in the DAU phase because you're going to keep coming back to it.

	A major component of the DAU phase, especially the understanding aspect is exploratory data analysis or EDA. On its own, exploratory data analysis can be several months work. By definition, EDA is exploratory and there's no limit to the amount of exploration you might want to do (but) it's very important to have some way of limiting how much time you spend doing EDA. There's a lot of value in EDA and so its easy to spend a lot of time here but be careful. It's easy to push EDA well beyond the needs of the machine learning algorithm without even realizing it. Of course, it's important to do good exploration but you should have a rough time limit to EDA so that you can actually start on the machine learning itself. EDA will teach you about different characteristics of the data as well as different anomalies in the data. Associated with this, is feature engineering in which you identify the features in the data that are irrelevant and don't add any information.

	After refining your raw data, the volume of data will be smaller than the original raw dataset. Your cleaning and processing phases generally mean throwing away a lot of useless, we hope, data.


	The DAU stage might inform you that the original problem you are trying to solve is wrong or that, you can't solve it.


Metadata:
	Metadata is simply data about data. This means a description and context for the data. Metadata helps organize, find, and understand data. It turns out that the data alone without the context of how it was collected, stored, processed, and monitored isn't nearly sufficient for responsible Machine Learning.

	It's important to know what metadata is available in the files you have so you can make an informed choice.


	If you consider a spreadsheet, everything beyond the data in the cells including those columns and headings is really metadata. Spreadsheets contain a few standard metadata fields such as table headers and column headers that can remain when they're exported to a standard file format such as CSV. Depending on the program, they may have many others like [inaudible] user comments, or inserted tables. Of course, one of the powerful features of spreadsheets are that you can do calculations right inside the cells. It's important to know when a number has been entered directly or calculated based on some formula, and that's metadata. Whether in spreadsheets or not, it's often critical to know how the data was collected.

	We've talked about bias in data, and you can't tell from the data itself whether or not it accurately represents the population you care about. So for any sample data, you need to keep track of how the sampling was done. Similarly, how well was the data collection done? Were the readings calibrated? We're professionals making a judgment? We've talked about the importance of labeled data but knowing where the labels came from is almost as important as the labels themselves. Did experts supply them? Where they cross-checked and triple-checked then validated against a Professional Standard? Or were they crowd sourced or filled in by some random volunteer?

	Using metadata you can better understand your data itself and decide what features to include. It also gives you insights into the quality of the data and how to resolve conflicts. Dealing with missing values is another important task that metadata really helps with.

	Basically, metadata is necessary to give you context. Otherwise, it's just numbers.

	As an example, here Amii, we encountered thevalue of metadata on a project where we wereinvestigating pump performance. A faulty sensor had been removed and replaced witha working sensor, but that new sensorhad a unique label even though it wasfunctioning in the same role. So the column representingthe original sensor shifted from validnumbers to all zeros, and a totally unrelated sensors spontaneously startedshowing readings partway through the data. In this case, ourexploratory data analysis identified the strangejump in values. Consultation withthe domain experts revealed what had happened, and we were able to manuallycombine those fields. Without having themetadata to identify that these sensors wererelated to the pumps, that anomaly would have been lost in the gigabytes of data. If we had treated that data as two different features, it would have been extremely detrimental for our learning system. Metadata was the key component of the project that helped us work with more than 200 different sensors. In another example we worked on predicting prices for a commodity. Although the column was labeled as price, it wasn't actually the price. The data stored there was the differential compared to a base price. The base price changed frequently on an unknown schedule according to supply and demand. Since we were trying to predict price having an unknown arbitrary offset made the prediction problem a little more difficult. Since the associated metadata didn't indicate how price was calculated, it took quite a few conversations and a little bit of work to get the data we needed. One last example, we're often dealing with data logs from years back and most data logs do some kind of summarization. It's really important to know what rules were used to summarize. When sensor readings that were originally taken every minute only have 12 records for a day, how are those chosen? Averaged over fixed time intervals, median, or mode, or only updated when the values changed. All of these have different implications for how you deal with data alignment and imputation

	The identification and curation of metadata is an important process. It's not really possible to have the one right answer for what data about data you need to keep. The most important thing to keep in mind when collecting data is, what additional contexts can I provide? How will it keep it with the data itself? When you're looking at different data sources, pay attention to the metadata as well as the raw material. Well curated metadata can be crucial for machine learning success.

We can manually identify a bunch of features that are appropriate for the question we care about, but we can also use various machine learning techniques to automatically find features. This process of transforming raw data into a more useful form is sometimes known as representation learning, and there are lots of standard feature extraction practices. For example, turning a document into a list of numbers representing the count of every possible word. feature selection and engineering since it's often the key to machine learning success. Remember that when determining features, you can manipulate the data in many ways. You can scale, discretized, combine the data. You can also filter out data that you know is irrelevant to your problem. Feature engineering is often a key to success and the best way to provide the machine learning process with a boost.


decision trees can be effective with a relatively small dataset, but neural nets need a very large amount. KNN can work well with a fairly small amount of data, but you need to keep that data in memory and accessible whereas other algorithms don't need further access to the training dataset once the training phase is done.

Quality Data:
	check in your data:
		missing data (data imputation)

		If we are not completely sure if missing values in a feature like "column_n" are MCAR, then there might be some information hidden in the missing-ness itself. So, it would be a good idea to add a feature to the DataFrame which indicates whether the value for a feature ("column_n" here) was missing prior to impting the missing values for that feature.

			1- MCAR (missing completely at random)
				there's no correlation between those missing values and the observed data.
				it is uncomman.
			2- MAR  (missing at random)
				there's still some randomness in what values are missing but it relates to the data you actually do have. There's some systematic difference and the missing values are influenced by something else present in the data. Something in the data you do have correlates with the values that are missing. So if you simply throw away the incomplete data, you're changing the distribution of your training data, maybe in some important way.
			3- MNAR (missing not at random)
				This is the trickiest case. It means there's a consistent reason why that particular data is missing and it's directly related to the value itself. This comes up in surveys when there's some question a person might not want to answer because of the answer. If you simply ignore these values, you're likely to severely bias your results. These missing values are the most difficult to fill in because they hold important information that you can't necessarily recover from the data you have. Algorithms that use such data need to accommodate missing values as part of the process.

			imputation:

				(Now, let's handle the missing values for "Mode". This is, however, a categorical feature and we cannot do a mean, median, mode, ... imputation. What we can do is impute with things like the majority category (there are other options as well, depending on the situation). However, we may not need do that since we are encoding the categorical variable into one-hot encoding. If we add another feature, treating the missing-ness as a category in itself, we can have a dataset that has all its values in numbers and we don't have to provide an imputed number. Actually, imputed numbers are inaccurate information and will not help but sometimes hurt our discrimination power. In the case of numerical features, we have no way but to provide a number but this is not the case with categorical feature. The binary missing-ness feature we add is sufficient	) (from wee3 programming assingment)

				1- imputation by constant:
					That means that all of the missing values of a feature will be filled in with the same value (0, mean, median, trimmed mean, mode .. etc) and it won't be dependent on any other feature.

					This can be sensible, but watch out for cases where you might find that the mean is skewed because of a few very large values. If that happens, you might want to use the median of the observed values or you could similarly use the most frequently occurring value of that feature.
				
				2- imputation by multiple values:
					It might not be helpful to just have random imputation. What if instead of a mean value on all the values of a feature, we use mean values pertaining to a specific value of another feature..... eg: <age> me jo value missing h us observation me <gander> kya h, (for example: wo 2 h) to ham srif wo values filter karen gy jin ka <gander> 2 ho ga or <age> missing na ho. or phir resulting df k <age> ka koi statistic (mean, median, mode ... ) ly kar us missing value ko fill karen gy.

					So why not take that further and use all the features that we have to fill in the missing values? We can do that and we can use some of the machine learning algorithms. Let's start with how we use k-nearest neighbors to fill in the missing values. From our previous example if we need to fill in the height value, we can use a distance metric and using any number of other features in this case, let's say we only take the age, we can find the nearest neighbor of the datapoint whose value is to be filled and fill it in based on the value of the neighbor. If more than one neighbor is considered, you can take either a mean of the values of the neighbors or a weighted mean based on the distance of the neighbor, for continuous values, or take the most frequent for categorical features. You could also use regression to impute values. You use a regression model based on the observed features and then predict the value for the unobserved feature. The predicted value is now what we use to fill in the missing numbers. This method of using regression models and using the exact value of the prediction is known as deterministic regression imputation. A variation of this is to add a small random value to the predicted value and use that to replace the missing value. This is the known as stochastic regression imputation. We've seen methods that can be applied to IID data but what if the data instances are correlated? Luckily, we have methods we can use for interdependent data as well. Let's take the example of temporal data where the value of each data point is related to the value of previous data points. For some applications, you can make an assumption that the values might remain the same or closer to the value that's been recorded the last. In that case, we can use the last value and fill in the missing values until there's another observed occurrence. This is called forward fill, or sometimes we want to use the future value to fill back the previous values. In that case, we're doing back fill. In some cases of temporal data, that domain might allow us to make assumptions that a specific feature is going to be increasing or decreasing. For example, if we have a temperature reading for each minute and we have five minutes of missing values, there's a good chance that the temperature reading for those missing minutes could just have been increasing or decreasing regularly. In these cases, we can use linear interpolation where we fill in the missing values by drawing a line that connects the last observed value to the next observed value and the values of the missing timestamps are drawn from that line. 

				What method you use to fill in missing values depends on the reasons the data is missing as well as the characteristics of the data itself. Often how you do the data imputation has a significant effect on the performance of your model. So you're faced with the usual mix of knowing your data, understanding your domain, and carefully paying attention to how your learning data relates to your operational data. Other methods exist and of course this is yet another active area of research. 

		duplicated data:
			Duplicate data also affects the quality of the data. Identifying and removing duplicates can be tricky. In the best case, they're obvious. But sometimes it's a bit harder to know when a duplicate will skew our analysis or mislead our models when it's from errors in the data process and not a real artifact of the data. Identifying and removing duplicates has to be done with care.
		corrupted records:
			remove corrupted records. But again, how to identify actual corruption in the record has to be handled by domain experts.
		outliers:
			Domain expertise is usually required to define the bounds of legitimate values.
		numeric:
			same unit (spcially when final data is obtained from joininig of multiple sources)
		catagorical:
			typos (typing variance)
		We may choose to correct those errors manually if possible. Otherwise, we'll need to remove them.

		Our next concern is to make sure that the data is valid, meaning the data we have indeed matches the requirements and structure imposed by the business problem. If the data doesn't match the problem, there's no point in improving quality. When we identify that the data is appropriate for the task at hand, we can move on to check that the data is accurate. It may pass all our structured checks but still not represent the events we care about, or not come from the same distribution as the operational data. Automated checks that compare the statistical profile of the learning data to the profile of the operational data can catch some things but nothing's perfect. This is another case where you have to use domain expertise and likely have external quality assurance on your data collection process. Another aspect of ensuring the data is appropriate is whether or not you're data is timely. Working with legacy data or outdated data could result in outdated trends and patterns that don't reflect current or future events being discovered by our learning algorithm. It's yet another way that the data generating distribution we care most about, the operational data, can differ from the data we have. Finally, to assure the quality of our dataset, we should assess how consistent it is. Especially, when integrating data coming from various sources, we have to assess whether the units are consistent or whether the numerical feature ranges are the same. For instance, we do not want data from one source to have been calibrated differently from another. a useful format, it's time for us to do quality checks to ensure that it's high-quality data.

The complexity of your model refers to how many knobs the machine learning algorithm is allowed to twiddle in its search for the best QuAM.

We can't know with certainty how complex a function needs to be to perfectly capture the relationship between features and answers. With perfect features, you might be able to learn from only a few examples. With more realistic features, you need more. When your features have a close relation to the correct answer, the learning algorithm can find that relation with only a little data. When your features are not closely and simply related to the correct answer, you need more data. Similarly, the more features it takes to accurately capture the relationship, the more data you need. If you have billions of examples to use to train your QuAM, you don't need to worry as much about good feature engineering. If you're like most of us and have a limited dataset, spending a lot of time and care on feature engineering is extra important. How much data you need to learn a good model also depends on the algorithm used to learn the underlying mapping function. Some families of learning algorithms allow more complex functions to be tested. Deep learning is able to find incredibly complex functions, which is part of why deep learning has been successful on so many tasks. Deep learning algorithms learn complex non-linear relationships, simple linear classifiers cannot. So if you're set on using deep learning or neural networks, you absolutely must have thousands of unique examples for every class you want to learn about. If a linear classifier achieves good performance with hundreds of examples per class, you may need tens or hundreds of thousands of examples to consistently get the same performance with a nonlinear classifier.

age could be used as ordinal data.

A feature can be :
	1- categorical where one possible value has no consistent or defined relation to another. For example, the similarity between the cat and dog label is the same as the similarity between the dog and most label. 

	2- The second feature type is ordinal. This means there's some ordering among the values of that feature, but the magnitude is not defined. Think if you want to pick the top ten golf players and their ranks are the values. You'll know that the top seeded player is higher in ranking compared to the second seeded player and the second seeded player is somehow above the third seeded player. But the magnitude in difference between those ranks is not defined. 

	3- The third type of data is continuous where the magnitude of the difference is reflected in the values itself. Consider the cost of an item as an example. The difference between an item marked at $5,000 and an item marked at $10,000 is the same as the difference between two items marked at $10,000 and $15,000. 


	Let's look more closely at categorical values. The categories themselves are not meaningful to the computer and be careful because categories can masquerade as numbers. Don't make the mistake of treating all numbers as continuous values. Consider an example where we have the serial number of items as one of our features. The serial number itself is actually a number but that doesn't mean the magnitude means anything. For categorical values, we need to transform them into features that can be understood by the machine without misleading the learning algorithm. A simple way would be to just replace our text categories of cat, dog and mouse with a unique number 1 2 and 3, respectively. We said everything has to be a number so this this works, right? Can you see any problem with doing this? Computers inherently think of numbers with order and magnitude. That means, computers always work in the mathematical space where adding two and three gives you five. If we transform the data into some unique integers, we get dog + mouse equal zebra and mouse- dog equals cat. This could be a fun game but it doesn't really help our learning algorithm. To avoid this, we use one hot encoding. In cases where we have more than one category for example, we can use a technique called multi hot encoding. Take the example of an image. Each image corresponds to a data instance and the same image could contain both dog and a cat. In such a case, the corresponding features will have a value of 1 and the others zero. Sum of the machine learning algorithms are not affected by categorical variable transformation. Tree-based methods are not affected by one hot encoding because they're able to make splitting decisions with the categorical value directly. Naive-based methods are also not affected because they're dependent on the count of the values in a class. But for most learning algorithms and even some implementations of tree-based methods, you have to do the encoding, where else find zebras resulting from cats and dogs. Let's take a look at the second data type, ordinal features. You can also use one hot encoding for ordinal numbers, and in some cases, that'll work fine. But if we do that, we lose out on the ordering which might be important information. If there's no way to recover a meaningful magnitude matching the ordering, we might choose to make our own. It will replace the ordinal features with a unique integer based on the ordering. In our example, you can replace the value of best as 1, good as 2 and bad as 3. This would maintain the ordering but does assume that the distance between best and good is the same as the distance between bad and good. But what can you do? At least in this case, you leave the learning algorithm with the ability to consider the ranking, even if the amounts are artificially equalized. The last type is continuous features. This is, in some sense, the primary type. The magnitudes already encoded so we don't have to transform them all the time. They might be ready to feed directly into the learning algorithm. But even here there are multiple reasons why you might want to transformation step: 
		1- min/max scaling, Min-max scaling is a technique where we throw away the absolute magnitude in favor of having a consistency in scale between the features. You transform each continuous features so that they all fit within a range and usually, keep that range consistent across all features. Typically, the range is from 0 to 1. Why would we want to do this? Let's look at an example where we have two features where one ranges between 0 and 10 and the other between 0 and 10,000. If we're using nearest neighbors or some other technique that looks at the distance between two points, the distance is going to be really dominated by the one feature with the larger range, which most likely isn't what we want. Scaling equalizes this, letting all features have a chance at equal weight in the computation of distance. The other reason we might want to scale generally is for saving memor. Scaling to small numbers means we can use data types that use less memory which means better memory utilization and faster computation. 
		
		2- There's another form of normalization called the mean normalization. Mean normalization centers values around 0 rather than between 0 and 1. Some algorithms actually assume your values are mean 0 and this is a simple way of ensuring that. 

		3- The third form of normalization we want to consider is zed normalization (standrization). This makes the distribution of values look like a normal distribution. Why we care? Think if you want to give bonuses to your employees based on the number of houses they sell each year. Selling houses is not necessarily linear. That means the effort required to sell four houses might not be exactly double the effort of selling two houses. It could be way more than that and you want to reward them accordingly. Zed score normalization is a way of normalizing that captures this variation. In terms of machine learning algorithms, the z-score normalization helps in giving more magnitude to extreme values and thus either penalizing or rewarding accordingly. It's worth mentioning here that not all methods are for all the algorithms. There's no one winner unless the task in hand the domain and understanding the features you have are very important before you apply anything.


Correct and high quality labels, whether class labels or real number targets, are key to training a good supervised learning model. Obviously, quality issues in the labels themselves can get in the way of training a good model. Even with the best labeling process, we have some chance of mistakes. Mislabeling can come from simple data entry errors or more profound confusion and even incompetence. Wrong labels will obviously mislead the learning algorithm, and can even break it entirely. If similar instances are mistakenly given entirely opposite outcomes. Outlier labels are another example for cases where early on small errors become huge issues later. For instance, in the regression problem when we have outliers, which are very far off. We might not be able to get QuAMs, which we can predict accurately with. If we use certain performance measures such as mean squared error, which are really sensitive to such cases. So, in those scenarios, we should be focusing on either resolving the outlier problem at the beginning. Or picking an evaluation metric which is robust to outliers. On the other hand, when we perform extraction, integration, transformation, and various other processing steps. Original features may change into something we didn't intend them to be. For instance, when we perform optical character recognition. If our OCR library isn't doing a good job, the text data we extract from the scanned images can provide many misleading data points and features. This leads to bad training data, and in return, we'll be training a bad QuAM with noisy data. When we realize something's wrong with our model performance. We always have to go back in the MLPL cycle and revisit the critical points where the error originated.


imbalanced data:
	having imbalanced classes in your learning data impacts the QuAM that results.
	One way you can handle the imbalanced classes problem is to change your evaluation metric (Precision, Recall, F1, ROC curve). you can also come up with a cost matrix or loss function that's a weighted combination of false positive and false negative errors with different weightings assigned to each type. Then you select the best classifier as the one that minimizes that cost matrix or loss function. You can also try to optimize something called Cohen's kappa. This measure adjusts for the imbalance of the classes by normalizing accuracy with the imbalanced ratio. ROC curves is another useful evaluation metric you can try for classification on imbalanced classes.

	You should also consider simply gathering more data. This is an important fix that's sadly overlooked many times. It's possible that the imbalance can be addressed with a larger dataset. This is especially true if the imbalance isn't inherent in the problem like classifying rare events, but is an artifact of your data collection. Even if not, more examples for minority classes may be especially important for resampling techniques, which we'll discuss shortly. Sometimes you can combine domain knowledge with data to improve classifier performance. For example, check out the advanced supplemental reading in fault diagnosis for chemical processes. If you combine process knowledge with historical data, you may get better performance, especially for rare faults. In addition, when you're working with imbalanced data you might want to try different learning algorithms, as different algorithms may be more or less suited to handling class imbalance. For example, decision trees will often perform well on imbalanced datasets, while others assume an even distribution. You may recall that decision trees learn a hierarchy of if else questions, and this can address both classes. If another technique is to synthesize data samples. You can create synthetic samples by randomly sampling features from data points in the minority class. We can also try resampling techniques to handle imbalanced data sets. One possibility is to add more copies of the minority class, which is called oversampling. You can replicate samples from the minority class randomly. Depending on the nature of your data, it might make sense to add a small amount of noise to the copies. If you're oversampling, it's important you split your data points into test and training sets before doing the oversampling. Otherwise, oversampling can lead to having exactly the same observations in both the test and training datasets. This allows our QuAM to simply memorize specific data points which leads to overfitting and poor generalization performance. Of course, you would never contaminate your test set that way. Undersampling means randomly removing some observations of the majority class. This is another technique for handling imbalanced data. The upside is our classes are more balanced. Downside is we're removing information that could have been valuable. This could cause underfitting and consequently poor generalization. After apply such resampling techniques, you'll have about the same number of data points for each class. It's important to make changes again on data samples of only the training set to ensure your QuAM performs well on unseen data. Last but not least, realize that using different sampling techniques can introduce bias into the data. After all, you're deliberately changing the distribution of the data. Sometimes this is a good thing. Sometimes it isn't. That's why a clean test set and solid domain knowledge are so important when deciding what technique to try.



ROC curve:
	ROC plot is a two-dimensional plot with the misclassification rate of one class, false positives on the x-axis, and the accuracy of the other class, true positives on the y-axis. An ROC plot not only preserves all performance-related information about a classifier, it also allows key relationships between the performance of several classifiers to be identified instantly by visual inspection.

	Cost curves are an alternative to ROC curves for visualizing the performance of binary classifiers. Cost curves are superior to ROC curves for visualizing classifier performance for most purposes. While they share many of ROC curves desirable properties, they can also show confidence intervals on a classifier's performance and visualize the statistical significance of the difference in the performance of two classifiers.


