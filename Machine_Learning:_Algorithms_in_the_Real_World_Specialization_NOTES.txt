Dimensionality reduction:
	Unsupervized learning / clustring technique, tries to reduce the number of features while losing as little information as possible.
	One simple technique for doing dimensionality reduction is principal componenet analysis (PCA)
		when we run PCA on data it can construct the n most infromative features. (these features are diffrent from existing data)


in Dicision Tree:
	restring the depth of the tree indirectly restricts the number of leaf nodes a tree can have. for example, having a maximum depth of three would allow at most 8 leaf nodes for that tree.

	overfitting can also be avoided by pruning decision trees. pruning can be done while building the trees, and this is called pre-pruning. Pruning can also be done after the decision trees have been built. this is called post-pruning. Pruning usually uses the cross-validation of validation scores to idendify which portions of the trees can be pruned. Pre pruning is also known as early stopping. and it's a faster method, but there are chances a could underfit the data as it gives just an estimate of when it should stop. post-pruning methods are not based on estimates, but rather actual values, and thedecision tree is pruned once it's completely built. 

	advantages of dicision tree:
		1- it is easiear to interpret then other models.
		2- can handle multi class classification
	diadvantage of dicition tree:
		they tend to be unstable. any little variation in the training data might result in totally diferent dicition tree. thus we use random forest. 

in Random forest:
	we train multiple dicision trees, eash with diffrest variables, (and bootstraping with replacement), then predict the most prediction. (eg: 50 trees, X ==> trees, 40 trees predict A, 5 predicts B, and 5 predicts C, we choose A as a prediction since it is most predicted)

	in the prediction phase, the observation goes into all trees, and most commont prediction choosen as prediciton for thtat observation.

	disadvantages:
		more dificult to interpret the Random forest then dicision tree. 

KNN:
	widely applied in many domains
	
	it is ofter useful to use it as a baseline test, no matter what your application is. 

	we need numerical representation for each variable (to campare distance, hence choose K closest)
	similaritly between two feature vectors can be measured by distance function.

	we don't need too larger K because that means listening to very distant neighbors. and lumping things together that are actually quite dissimilar. on the other hand, we don't want too smaller K, because then random meaningless local variations can have too much influence on our choice.

	(we know nearest neighbourhow graphically, but) computers can't glance and guess the way humans can, even in the two-dimensional case. (so we use distance functions to know distance. and distance functions require a numeric vectors)

	unlike most other classification methods, KNN is a type of lazy learning, Meaning that, there is no seperate training phase before classification. This does highlight one big downside ot thir algorithm, we have to keep the entire training data in memory. and as we get more examples it gets more and more expensive to computer which neighbors are nearest. for this reason, KNN usually works best on smaller data sets that don't have too many features. 

	There are two important decisions that must be made before building KNN model:
		1- value of K
			You can choose K somewhat arbitrarily, or could try it cross-validation to compe up with an optimal value for K.

			K should also be chosen such that there will be a clear majarity between classes.

			The choice of K is essential in building the KNN.
			in fact, k can be regarded as one of the most important factors influencing the KNN classification performance.

			K is smoothing parameter: 
				for any given problem a small value of K will lead to large variance in predictions. alternatively, setting K to a large vlaue, means you don't get a very customized answer, if k is the entire set of examples, you're going to always guess the majority class.

				K should be set to a value large enough to minimize the probability of misclassification, and small enough that it's actually a meaningful subset of the total number of examples.


		2- distance measure:

			(we have to scale our data before calculating the distance)
			a distance function maps two elements to some real number , the distance between them.

			There are 3 rules that must bold for valid distance measures. 
				
				1- only positive numbers
					distance(a,b) >= 0 # if a = b then distance(a,b) = 0
				2- order doesn't metter
					distance(a,b) = distance (b,a)
				3- the triangle inequality must hold (detouring from the staright line can only increase the distance)

			Q: There are many differny valid distance measures, what's the best? 
			Ans: as usual it depends on your problem

			Euclidean distance.

				Pythagorean theorem:
					sqrt(
						(x2- x1)^2 + (y2 - y1)^2
						)


					E(A,B):
						SUM = 0
						for j in range(1, m+1) # in python we start from 0
							SUM += (X(A,j) - X(B,J))^2
						sqrt(SUM)

				in Linear algebra:
					finding the magnitude of the diffrence vector between two points.
				
				it is genrelize to M dimensional space.


				it is also called (L2 NORM), L2 because we're squaring the values and norms is the technical term for the magnitude of a vector.

				Pehaps the most commonly used distance for machine learning algorithms.

				it's vary useful when our features are cotinuous, but there are some situations where Euclidean distance isn't quite right. As an example, if you're a taxi driver driving through the organized street blocks of a city then the straight line distance would
				force you to pass through buildings to get to your destination. So it's more accurate to consider what path you can take along the city blocks rather than straight line. In fact, we call it Manhattan distance.

			Manhattan distance:
				if we need to calculate the distance between two data points in a grid-like space. That grid-like space can be represented by what we call a Cartesian coordinate system. Cartesian coordinates describe locations by their distance along different axes. 

				For Manhattan distance, we take the differences along each of the axes and sum up those differences. This gives us the grid walking distances between two points, formally it's the sum of their absolute differences of their Cartesian coordinates. 

				distanceManhattan(A,B):
					SUM = 0
					for j in range(1,m+1) # in python we start from 0
						SUM += abs(  X(A,j) - X(B,j)  )
				Manhattan distance is better when we're dealing with high dimensional data or when you know, you can't go straight from point a to point b, then you might as well use the more appropriate measure. And because we're not squaring the difference like we did for Euclidean distance, we're not magnifying distances. This helps deal with outliers. 

				is also called L1 distance or L1 norm

				Euclidean and Manhattan distance measures are standard and useful measures, but they're only valid for continuous variables. If you have categorical or non continuous variables, the Hamming distance might be most appropriate. 

			Hamming distance only cares about differences, when the value's the same the distance is 0, if the values are at all different the distance is 1. It's easy to see that this works even for categorical features. The Hamming distance between two feature vectors is exactly the number of features that are different.

				distanceHamming(A,B):
					SUM = 0
					for j in range(1, m+1): # since in python we start counting from 0
						SUM += abs(  X(A,j) - X(B,j)  )

				if X(A,j) = X(B,j) ==> distanceHamming<j> = 0
				if X(A,j) != X(B,j) ==> distanceHamming<j> = 1



	with the power and simplicity, KNN also comes with downside of requiring mem ory for the training dataset and a fair amount of computation.



LOGISTIC REGRESSION:
	
	two steps:
		1- first linear regression
		2- logistic transfer function take linear function output and give some number(0,1)

	predict the probabilities that a data point belongs to a particuler class.

	it called <Logistic regression> because it's doing classification using a regression operation and a transfer function.
		transfer function:
			a function whose job it is to translate the output of one function into some other space. in this case the the trainsfer functions (which is logistic function here) takes the number reported by our regression model and transfer  it into a class label.
	the value it finds via the regression step can be thought of as the probability that a data poing belongs to a particuler class.

	The threshold can be adjusted based on the domin and whaterver bias you need for each class

	The logistic transfer function (or simply the logistic function) maps the entire real line and transform it into space 0 and 1.

	if we use square loss*** to evaluate the parameters of our logistic function(as in linear regression) that is not convex, since it is not convex we can't guarantee that we can get the botom of the hill it is actually the lowest possible point. we could get stuck in local minimum. so we get complementry loss, so that you DO get a convex loss landscape, these are called matching losses, if you have a maching loss for your particuler transfer function then the minimum point IS the global minimum which means you CAN find the best line. so once you solve the linear regression problem you can use the solution point and it will be the best solution after applying the logistic transfer as well. 
	logistic loss:
		squared error loss of the sigmoid function if the label is 0:
			y = -log( 1 / (1+e-f(x)))
		squared error loss of the sigmoid function if the label is 1
			y = -log( 1 - (1 / (1+e-f(x))))
	if you use this matching loss with a logistic transfer function you get a convex and smooth landscape. Now gradient decrease find your optimal model. 
	***	squared error loss of the sigmoid function if the label is 0:
			y = ( 1 / ( 1 + e-f(x))^2 )
		squared error loss of the sigmoid function if then label is 1:
			y = ( 1 - ( 1 / ( 1 + e-f(x))^2 ))
in Linear regression:

	Non Linear features and model complexity:
		
		to overcome underfitting we need to increase the complexity of the model. 

		Luckly, we can get complexity in Linear function through features enginearing.
		 	polynomial, square root, log
		
		polynomial:
		 	multiplies things together.
		 	the order(degree) of polynomial tell you how many times you multiply then.
		 	order2 polynomial = squared = X * X
		
		we create non Linear functions by adding or changing features that are non Linear functions of the orignal features.

		non Linear features does increase the complexity of the model which has sight affects. thre are some issuse we have thing about when we use non Linear features to add complexity to the model. 
			(much hight degree of polynomial feature causes overfitting, so be carefull)








	Linear functions have handy features like convexity and differentiability.  
	Goal: 
		find parameters (weights) that minimize the loss function.

	least squares AKA L2 loss function.

		this loss is useful because it is gives a shape that have a unique point for the minimum panalty(SSE).
		smooth and convex. which means it is going to have unique minimum point, we don't know what that minimum is, but using calcules we can find id.

		Q: is this always the panalty you want to use?
		A: NO.

		Q: we could just take the absolute value of diffrence (y^i - yi) to get the actual magnitude, why not do that?
		A: we do, and that is totally walid loss function, is is called <L1 loss> or <least absolute error. when we use gradient descent bot L1 and L2 are convex, but L1 has sharp corver <V shape> at minimum value (thus this is not smooth, thus it is not differentiable, we can't take the darivite of the L1 loss to find which parameter gives us the minimum panalty like we did with L2 loss function.> instead of smmoth curve like L2 <U shape>. 


	gradient descent:
		very commonly use in machine learning

		in gradient descent we rely on the property that our graph (loss vs slop) is smooth and continuous, so that we can take tha derivative, and convex so thre is no more than one minimum value.

		an iterative method, we start fron any point then make small adjustments in our perameters(in regression the coefficient/s) to bring us closer to an approximate solution. 
		we choose point where darivite is 0.
		require smooth function (so we don't use L1 loss function)

		We randomly pick a starting position and imagine placing a ball there. For minimizing loss, this amounts to assigning random parameters to our model. This is simply an initial guess, we have to start somewhere before we can improve it. Say our random initialization is peer. We hinted that the ball will roll down the pill somehow, but we can exactly calculate in what direction the ball will roll using the derivative or the slope of the tangent to the graph at this point. We'll use this derivative to take small steps towards the minimum of our graph. We have this mathematical notation for how to take a step or in other words, how we can tweak that parameter w. We update it by taking the current estimate then subtracting alpha times the derivative with respect to w of the loss function of the current estimate. Let's go through that bit by bit to see what's going on. Of course, we start at time zero, so w0 is our first guess for our model parameter. Now we want the ball to roll down the hill, but we need to know which direction is down. Hurray for calculus, that's the gradient of the loss function, the derivative with respect to w. So that's what direction to step, but how far do we adjust w? We need to know how large a step to take. There's a parameter for this known as the learning rate or step size. Here in our equation, we've represented this with the Greek letter alpha. It's your choice what learning rate to use, but it's some number greater than zero or you wouldn't learn anything and smaller than one or you'll over correct. Multiplying the learning rate by the gradient determines how much we should change the current position on the horizontal axis. Remember, this is the value for our models parameter w. Notice that the steeper the gradient, the bigger the change and vice versa. When the gradient is less steep, the learning rate multiplied by the gradient will be smaller. So the amount we update our position by will be smaller. This is true even if we have a rather large learning rate. What is a good learning rate to use then? It turns out this is a hard question to answer. A good learning rate is one that's not too big because you'll end up stepping past local minima and you never settle on a good parameter value. But can't be too small or it will take forever to converge, to settle down near the best value. So we want to have alpha that's as big as possible so that it will learn quickly but not one that's too big and diverges. Some techniques have decreasing alphas so that you can start big but settled right down, and depending on the rate of decrease, you can actually guarantee convergence. But it's usually simplest to sacrifice optimal convergence for our convenience and use some small alpha. The best alpha depends entirely on your particular problem. When in doubt, try 0.1, if that seems unstable, try 0.01 or start with a default set by your particular library. Enough about alpha. Let's look at the key operation in this equation, subtraction. We're taking our current position and subtracting the learning rate multiplied by the gradient. This gives us our new value which we really want to be closer to the optimal value. So why subtraction? Notice that at time t, we're here on our graph. In our 2D example, we can see that in order to get closer to the minimum, we want to step to the right. Stepping to the right is equivalent to increasing w along the horizontal axis. Notice that at this position the gradient is negative. The learning rate is always a positive number so subtracting a negative number from our current position moves us to the right. Similarly, if we'd been located here at time t, we'd want to move left to get closer to the minimum. Our gradient at this point is positive and so subtracting it from our current position will make w smaller moving us to the left. To summarize, because we've made sure our loss graph is smooth and convex the derivative or gradient always exists and always points away from the minimum point. In order to get closer to the minimum, we move against the gradient. This is why we have a minus sign in our update equation and also why we call it gradient descent. The process is iterative, so after subtracting the gradient multiplied by our learning rate, we have a new position for our ball. We calculate the gradient at this new point, multiply it by our learning rate and subtract that from the current position. Repeat this process over and over until we're not changing our position very much because the gradient has gotten so small. This lets us converge on or close to the minima. What does convergence mean? Remember, that local minimums happen when the derivative is equal to zero. As we step closer and closer to the minimum, the derivative will get closer and closer to zero as long as we're not overstepping. When we arrive at the minimum, the change will be zero and so our parameter value won't change anymore. We converge to the position where the minimum occurs.


	Across all loss functions, there's one crucial property that each must
	possess if you want to guarantee you're finding the global minimum. This
	property is convexity. The function is called convex if, for any two points
	on the graph of the function, the line segment connecting those two points
	lies above or on the graph. If you think of functions as describing some
	surface, you have to be able to attach the ends of a string anywhere along
	the surface and pull that string flat. If there's some way to attach the
	string where you can't make it absolutely flat without intersecting the
	surface, then it's not a convex surface. Why is convexity important?
	Convexity guarantees that our function has no more than one local minimum
	value.

		When our graph is not convex, we don't know for sure we've found the absolute smallest minimum value unless we explore the entire space. Whereas, no matter where we drop that ball on a convex graph, we'll reach the same final point at the only local minimum. That's the key, convex functions have at most one minimum value. So regardless of whether or not you use derivatives, if your function is differentiable or some other iterative numerical solver, if your function is not differentiable, when your function is convex, you're guaranteed to find the optimal solution in that hypothesis space. So one local minimum means we're able to find the smallest penalty value for our loss function. And if our loss function is differentiable as well as convex, this means we may be able to find an optimal function by setting the derivative of our loss function to zero, as we did in the case of L2 loss. 


bias variance tradeoff:
	
	complexity is not always good. we need our model to be complex enough to capture the relevent patterns in the data but too complex menas heigh variance and it is not ganralized well. 

	as we complex our model; in some range of complexity the bias and variance both decrease, after that poing the bias still decrease and variance increase (means: we begin overfitting), we have to choose optimal model.

	(very low bias: overfitting)

	one way to ovoid overfitting is to refuse to allow complex models.

	if we completely remove bias by adding complexity in the model, we have no bias but we are overfitting, means: the variance is too heigh. as we simple our model the bias is height, but variance is low.

	more complex the model: the low bias and heigh variance.

	algorithms with high bias will underfit the trainig data as well as perform poorly on the test/validation data.

	variance in the model is a bit hard to measure. you can look at how much you output cahnge is when you repeat the learning process under differen sample of the training data. you can also take advantage of relationship between complexity and variance, and use tricks incurage simple models.

	Reguralization A systamatic way of macking of tradeoff between bias and variance.

	learning curves can also be used dignose your model performance. 



Reguralization:
 
	A systamatic way of macking of tradeoff between bias and variance.
	A way to avoid too complexity in the model.
	
	when a learning algorithm has an objective function that only penalizes mistakes as with the loss functions we've already described, there's no reason for it to choose a simple model. So as usual, we need a mathematically precise way to measure simplicity if we're going to ask the computer to consider it. We want to modify our objective function with some criteria that rewards simplicity as well as accuracy. Just like we converted best line to least bad by penalizing mistakes, we can convert most simple to least complex and add a penalty for complexity. This is the idea behind regularizers. A regularizer is an extra mathematical term added to the objective function which penalizes complexity. Now the objective you want the learning algorithm to minimize is a combination of the penalty for making mistakes as well as the penalty for using a complex model.

	As your model gets more and more complex and more and more fitted to the training data by using higher and higher degree polynomial features, you can observe that the weights are getting larger and larger. The magnitude of weights looks like a good proxy to measure complexity and that's often what we use. We use a regularization term to penalize extreme values for our parameters. How do we measure extreme values? We sum them up. As usual, since we care about the magnitude and would prefer to keep things differentiable, the L2 norm is our friend. Remember that the L2 norm squared is the sum of the components squared. It's the magnitude of the weight vector. Remember also that the L2 norm penalty makes extreme values cause even more lost than an L1 norm penalty would because it's squares those values. But we want a little more control over this term because we might want to explore how much we should penalize the weight values. We do that by turning to another common trick in machine learning, introduce a coefficient, in this case lambda. Lambda acts as an adjustment knob to control the relative strength of carrying about simplicity compared to carrying about loss. With lambda equals zero, your objective function simply considers loss like before.

	As you turn this knob up to increase lambda, you penalize the learning algorithm more and more for complexity via the size of the weights. This means, your higher degree model gets simpler and simpler through having less and less extreme values which has a slight cost in terms of increasing average loss on the training data, but there is a benefit. You reduce the variance in your model generalization. This usually improves performance on test data at least up to a certain point. Lambda is another one of those smoothing parameters. It lets you control the trade-off between complexity and generalization between variance and bias in your model creation. However, this is not the only thing we get with regularizers. In some cases you can get interesting benefits by using different kinds of regularizers. For example, using an L1 norm of the weight vector as your regularizer term causes many models to try to find a sparse solution. This means the algorithm will try to find a model whereas many of the parameters as possible are equal to exactly zero instead of just being small. This means you can discard many of the features and use a smaller subset of features in your model.

	Regularizers also provide other benefits when combined with different kinds of specific losses. For example, when you combine L2 regularizers with SVM classifiers, we get the ability to use kernels. Kernels let you classify data points in a complex and sometimes infinite-dimensional space of nonlinear features.

	The concept of regularization can be applied almost anywhere

	Linear regression with an L2 regularizer is called ridge regression.
	Linear regression with an L1 regularizer is also called Lasso regression.



 Neural network:
 	neuron is basic component of Neural network.
 	each neuron take some numbers of input and give a (single) number as output.
 	typically each neuron actualy envolve two saparate simple functions. 
 		1- a linear function.
 			because linear functions are differentiable and convex, it realy easy to calculate, out hypothesis space is too small when we only look at linear function. 
 		2- then a non linear transfermaton(AKA activation function).

 		multiplies its weights against the input values and returns the sum. then that value is passed to activation function, and outcome is some other number, this output is the final output from this neuron.

 	Neural network learning these weights using gradient decrease.

 	output of on layer is a input of next layer.
 	input layer , following by hidden layer/s , and the output layer(which is actualy a predictions)
 	in between the input layer and output layer can be ANY number of hidden layers. 
 	the layers can be loop back from themselves, but dafault is the feed forward Neural network; that feeds each hidden layer with the output of the layer before.
 	one we have multiplied hidden layers the Neural network is called deep Neural network. and the process of learning is Deep learning. 

 	sigmoid function = logistic regression
 		take inputs from privious layer, run linear regression, the output of linear regression go into logistic function, and give some output(0,1)

 	with then Neural network we don't need to optimize each neuron saparately, 	it isn't clear how we could optimize each neuron saparately. we know what we want the entire network to do. but we DO know we need each layer to output values that ware makes the work of next layer easier. 

 	Neural nets can't be used for everything, we can't explain the results. 

 	the huge hypothesis space explored by neuron networks created variance, this is why the lots of data is incredibly important.


SVM (suport vecter machine):
	
	the combenation of the loss and the transfer function has to be a convex surgate. 
	
	hing loss function:
		this function solves some of the problem created by other loss functions, and can be used to turn the power of regression towards classification. 

		lossMisclassification(y^i, yi) =  1 if yi*y^i=-1, and 0 if yi*y^i=+1
			+1 * -1 = -1
			-1 * +1 = -1
			+1 * +1 = +1
			-1 * -1 = +1
		with this classification we treat every missclassification equqly bad, but may be some missclassifications are worse then others. remember we are trying to decide how bad a linear saperation of the two classes is. may be we can impose stronger panalties for samples found deeper in taratory of the other lass. in fact we wouldn't mind the margen for errors. so even correctly classified samples  but are really close to the line almost missclassified could increase the panalty. 

		we compute the panalty:
			z^i = xiT w
			and then use sign function to determine which side of the line taht particular example falls on.
			y^i = sign(z^i)		

			if z^1 = 0: 
				then that example x is exactly on the decition boundry.

			so in the linear part of our function the points further from the line will be mapped to increasingly positive or increasingly nagative numbers, depending on which side of the line they are on.  we can use that number (z^) as a notion for signed distance from the line. we can use it to create a panalty that magnified by distance from the linear sapareter, if we take this notion of signed distance (Z^) and multiply by the correct lable (y) we get that number that posative if the sample falls in currect side of the line. or nagative if the point falls in wrong side of the line. 
			big positive: deeply on correct side.
			big nagative: deeply on wrong side.
			realy small when it is quit close to dicision boundry. 

			we want to penalizes nearly wrong classification create some margen fro error arround the dicision boundry. we typically give ourselves margen of 1, we want to penalizes plasing the line less then 1 unit away from a point evan if it is correctly classified. for this we just need to subtract our y*z^ from 1 <1-y*z^>, this value will be posative for any point that encroaches on that margin, or is incorrectly classified by our line. 
				this way not only incorrectly classified points get panalty but correctly classified points that are in that margin space around the dicision line will also recive small panalties. 

				how ever if use this <1-y*z^) as our loss  the correctly classified points will get a nagative panalty which is wrong. we can fix it by filtring losses less then 0, thir bring us the final formula for hing loss:
					max{0, 1-y*z^}  
			