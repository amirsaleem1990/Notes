Dimensionality reduction:
	Unsupervized learning / clustring technique, tries to reduce the number of features while losing as little information as possible.
	One simple technique for doing dimensionality reduction is principal componenet analysis (PCA)
		when we run PCA on data it can construct the n most infromative features. (these features are diffrent from existing data)


in Dicision Tree:
	restring the depth of the tree indirectly restricts the number of leaf nodes a tree can have. for example, having a maximum depth of three would allow at most 8 leaf nodes for that tree.

	overfitting can also be avoided by pruning decision trees. pruning can be done while building the trees, and this is called pre-pruning. Pruning can also be done after the decision trees have been built. this is called post-pruning. Pruning usually uses the cross-validation of validation scores to idendify which portions of the trees can be pruned. Pre pruning is also known as early stopping. and it's a faster method, but there are chances a could underfit the data as it gives just an estimate of when it should stop. post-pruning methods are not based on estimates, but rather actual values, and thedecision tree is pruned once it's completely built. 

	advantages of dicision tree:
		1- it is easiear to interpret then other models.
		2- can handle multi class classification
	diadvantage of dicition tree:
		they tend to be unstable. any little variation in the training data might result in totally diferent dicition tree. thus we use random forest. 

in Random forest:
	we train multiple dicision trees, eash with diffrest variables, (and bootstraping with replacement), then predict the most prediction. (eg: 50 trees, X ==> trees, 40 trees predict A, 5 predicts B, and 5 predicts C, we choose A as a prediction since it is most predicted)

	in the prediction phase, the observation goes into all trees, and most commont prediction choosen as prediciton for thtat observation.

	disadvantages:
		more dificult to interpret the Random forest then dicision tree. 

KNN:
	widely applied in many domains
	
	it is ofter useful to use it as a baseline test, no matter what your application is. 

	we need numerical representation for each variable (to campare distance, hence choose K closest)
	similaritly between two feature vectors can be measured by distance function.

	we don't need too larger K because that means listening to very distant neighbors. and lumping things together that are actually quite dissimilar. on the other hand, we don't want too smaller K, because then random meaningless local variations can have too much influence on our choice.

	(we know nearest neighbourhow graphically, but) computers can't glance and guess the way humans can, even in the two-dimensional case. (so we use distance functions to know distance. and distance functions require a numeric vectors)

	unlike most other classification methods, KNN is a type of lazy learning, Meaning that, there is no seperate training phase before classification. This does highlight one big downside ot thir algorithm, we have to keep the entire training data in memory. and as we get more examples it gets more and more expensive to computer which neighbors are nearest. for this reason, KNN usually works best on smaller data sets that don't have too many features. 

	There are two important decisions that must be made before building KNN model:
		1- value of K
			You can choose K somewhat arbitrarily, or could try it cross-validation to compe up with an optimal value for K.

			K should also be chosen such that there will be a clear majarity between classes.

			The choice of K is essential in building the KNN.
			in fact, k can be regarded as one of the most important factors influencing the KNN classification performance.

			K is smoothing parameter: 
				for any given problem a small value of K will lead to large variance in predictions. alternatively, setting K to a large vlaue, means you don't get a very customized answer, if k is the entire set of examples, you're going to always guess the majority class.

				K should be set to a value large enough to minimize the probability of misclassification, and small enough that it's actually a meaningful subset of the total number of examples.


		2- distance measure:

			(we have to scale our data before calculating the distance)
			a distance function maps two elements to some real number , the distance between them.

			There are 3 rules that must bold for valid distance measures. 
				
				1- only positive numbers
					distance(a,b) >= 0 # if a = b then distance(a,b) = 0
				2- order doesn't metter
					distance(a,b) = distance (b,a)
				3- the triangle inequality must hold (detouring from the staright line can only increase the distance)

			Q: There are many differny valid distance measures, what's the best? 
			Ans: as usual it depends on your problem

			Euclidean distance.

				Pythagorean theorem:
					sqrt(
						(x2- x1)^2 + (y2 - y1)^2
						)


					E(A,B):
						SUM = 0
						for j in range(1, m+1) # in python we start from 0
							SUM += (X(A,j) - X(B,J))^2
						sqrt(SUM)

				in Linear algebra:
					finding the magnitude of the diffrence vector between two points.
				
				it is genrelize to M dimensional space.


				it is also called (L2 NORM), L2 because we're squaring the values and norms is the technical term for the magnitude of a vector.

				Pehaps the most commonly used distance for machine learning algorithms.

				it's vary useful when our features are cotinuous, but there are some situations where Euclidean distance isn't quite right. As an example, if you're a taxi driver driving through the organized street blocks of a city then the straight line distance would
				force you to pass through buildings to get to your destination. So it's more accurate to consider what path you can take along the city blocks rather than straight line. In fact, we call it Manhattan distance.

			Manhattan distance:
				if we need to calculate the distance between two data points in a grid-like space. That grid-like space can be represented by what we call a Cartesian coordinate system. Cartesian coordinates describe locations by their distance along different axes. 

				For Manhattan distance, we take the differences along each of the axes and sum up those differences. This gives us the grid walking distances between two points, formally it's the sum of their absolute differences of their Cartesian coordinates. 

				distanceManhattan(A,B):
					SUM = 0
					for j in range(1,m+1) # in python we start from 0
						SUM += abs(  X(A,j) - X(B,j)  )
				Manhattan distance is better when we're dealing with high dimensional data or when you know, you can't go straight from point a to point b, then you might as well use the more appropriate measure. And because we're not squaring the difference like we did for Euclidean distance, we're not magnifying distances. This helps deal with outliers. 

				is also called L1 distance or L1 norm

				Euclidean and Manhattan distance measures are standard and useful measures, but they're only valid for continuous variables. If you have categorical or non continuous variables, the Hamming distance might be most appropriate. 

			Hamming distance only cares about differences, when the value's the same the distance is 0, if the values are at all different the distance is 1. It's easy to see that this works even for categorical features. The Hamming distance between two feature vectors is exactly the number of features that are different.

				distanceHamming(A,B):
					SUM = 0
					for j in range(1, m+1): # since in python we start counting from 0
						SUM += abs(  X(A,j) - X(B,j)  )

				if X(A,j) = X(B,j) ==> distanceHamming<j> = 0
				if X(A,j) != X(B,j) ==> distanceHamming<j> = 1



	with the power and simplicity, KNN also comes with downside of requiring mem ory for the training dataset and a fair amount of computation.



LOGISTIC REGRESSION:
	it called <Logistic regression> because it's doing classification (using) a regression operation and a transfer function.
		transfer function:
			a function whose job it is to translate the output of one function into some other space. in this case the the trainsfer functions (which is logistic function here) takes the number reported by our regression model and transfer  it into a class label.
	the value it finds via the regression step can be thought of as the probability that a data poing belongs to a particuler class.

	The threshold can be adjusted based on the domint and whaterver bias you need for each class

in Linear regression:

	Non Linear features and model complexity:
		
		to overcome underfitting we need to increase the complexity of the model. 

		Luckly, we can get complexity in Linear function through features enginearing.
		 	polynomial, square root, log
		
		polynomial:
		 	multiplies things together.
		 	the order(degree) of polynomial tell you how many times you multiply then.
		 	order2 polynomial = squared = X * X
		
		we create non Linear functions by adding or changing features that are non Linear functions of the orignal features.

		non Linear features does increase the complexity of the model which has sight affects. thre are some issuse we have thing about when we use non Linear features to add complexity to the model. 
			(much hight degree of polynomial feature causes overfitting, so be carefull)








	Linear functions have handy features like convexity and differentiability.  
	Goal: 
		find parameters (weights) that minimize the loss function.

	least squares AKA L2 loss function.

		this loss is useful because it is gives a shape that have a unique point for the minimum panalty(SSE).
		smooth and convex. which means it is going to have unique minimum point, we don't know what that minimum is, but using calcules we can find id.

		Q: is this always the panalty you want to use?
		A: NO.

		Q: we could just take the absolute value of diffrence (y^i - yi) to get the actual magnitude, why not do that?
		A: we do, and that is totally walid loss function, is is called <L1 loss> or <least absolute error. when we use gradient descent bot L1 and L2 are convex, but L1 has sharp corver <V shape> at minimum value (thus this is not smooth, thus it is not differentiable, we can't take the darivite of the L1 loss to find which parameter gives us the minimum panalty like we did with L2 loss function.> instead of smmoth curve like L2 <U shape>. 


	gradient descent:
		very commonly use in machine learning

		in gradient descent we rely on the property that our graph (loss vs slop) is smooth and continuous, so that we can take tha derivative, and convex so thre is no more than one minimum value.

		an iterative method, we start fron any point then make small adjustments in our perameters(in regression the coefficient/s) to bring us closer to an approximate solution. 
		we choose point where darivite is 0.
		require smooth function (so we don't use L1 loss function)

		We randomly pick a starting position and imagine placing a ball there. For minimizing loss, this amounts to assigning random parameters to our model. This is simply an initial guess, we have to start somewhere before we can improve it. Say our random initialization is peer. We hinted that the ball will roll down the pill somehow, but we can exactly calculate in what direction the ball will roll using the derivative or the slope of the tangent to the graph at this point. We'll use this derivative to take small steps towards the minimum of our graph. We have this mathematical notation for how to take a step or in other words, how we can tweak that parameter w. We update it by taking the current estimate then subtracting alpha times the derivative with respect to w of the loss function of the current estimate. Let's go through that bit by bit to see what's going on. Of course, we start at time zero, so w0 is our first guess for our model parameter. Now we want the ball to roll down the hill, but we need to know which direction is down. Hurray for calculus, that's the gradient of the loss function, the derivative with respect to w. So that's what direction to step, but how far do we adjust w? We need to know how large a step to take. There's a parameter for this known as the learning rate or step size. Here in our equation, we've represented this with the Greek letter alpha. It's your choice what learning rate to use, but it's some number greater than zero or you wouldn't learn anything and smaller than one or you'll over correct. Multiplying the learning rate by the gradient determines how much we should change the current position on the horizontal axis. Remember, this is the value for our models parameter w. Notice that the steeper the gradient, the bigger the change and vice versa. When the gradient is less steep, the learning rate multiplied by the gradient will be smaller. So the amount we update our position by will be smaller. This is true even if we have a rather large learning rate. What is a good learning rate to use then? It turns out this is a hard question to answer. A good learning rate is one that's not too big because you'll end up stepping past local minima and you never settle on a good parameter value. But can't be too small or it will take forever to converge, to settle down near the best value. So we want to have alpha that's as big as possible so that it will learn quickly but not one that's too big and diverges. Some techniques have decreasing alphas so that you can start big but settled right down, and depending on the rate of decrease, you can actually guarantee convergence. But it's usually simplest to sacrifice optimal convergence for our convenience and use some small alpha. The best alpha depends entirely on your particular problem. When in doubt, try 0.1, if that seems unstable, try 0.01 or start with a default set by your particular library. Enough about alpha. Let's look at the key operation in this equation, subtraction. We're taking our current position and subtracting the learning rate multiplied by the gradient. This gives us our new value which we really want to be closer to the optimal value. So why subtraction? Notice that at time t, we're here on our graph. In our 2D example, we can see that in order to get closer to the minimum, we want to step to the right. Stepping to the right is equivalent to increasing w along the horizontal axis. Notice that at this position the gradient is negative. The learning rate is always a positive number so subtracting a negative number from our current position moves us to the right. Similarly, if we'd been located here at time t, we'd want to move left to get closer to the minimum. Our gradient at this point is positive and so subtracting it from our current position will make w smaller moving us to the left. To summarize, because we've made sure our loss graph is smooth and convex the derivative or gradient always exists and always points away from the minimum point. In order to get closer to the minimum, we move against the gradient. This is why we have a minus sign in our update equation and also why we call it gradient descent. The process is iterative, so after subtracting the gradient multiplied by our learning rate, we have a new position for our ball. We calculate the gradient at this new point, multiply it by our learning rate and subtract that from the current position. Repeat this process over and over until we're not changing our position very much because the gradient has gotten so small. This lets us converge on or close to the minima. What does convergence mean? Remember, that local minimums happen when the derivative is equal to zero. As we step closer and closer to the minimum, the derivative will get closer and closer to zero as long as we're not overstepping. When we arrive at the minimum, the change will be zero and so our parameter value won't change anymore. We converge to the position where the minimum occurs.


	Across all loss functions, there's one crucial property that each must possess if you want to guarantee you're finding the global minimum. This property is convexity.
		The function is called convex if, for any two points on the graph of the function, the line segment connecting those two points lies above or on the graph. If you think of functions as describing some surface, you have to be able to attach the ends of a string anywhere along the surface and pull that string flat. If there's some way to attach the string where you can't make it absolutely flat without intersecting the surface, then it's not a convex surface. Why is convexity important? Convexity guarantees that our function has no more than one local minimum value.

		When our graph is not convex, we don't know for sure we've found the absolute smallest minimum value unless we explore the entire space. Whereas, no matter where we drop that ball on a convex graph, we'll reach the same final point at the only local minimum. That's the key, convex functions have at most one minimum value. So regardless of whether or not you use derivatives, if your function is differentiable or some other iterative numerical solver, if your function is not differentiable, when your function is convex, you're guaranteed to find the optimal solution in that hypothesis space. So one local minimum means we're able to find the smallest penalty value for our loss function. And if our loss function is differentiable as well as convex, this means we may be able to find an optimal function by setting the derivative of our loss function to zero, as we did in the case of L2 loss. 


bias variance tradeoff:
	as we complex our model; in some range of complexity the bias and variance both decrease, after that poing the bias still decrease and variance increase (means: we begin overfitting), we have to choose optimal model.

	(very low bias: overfitting)

	one way to ovoid overfitting is to refuse to allow complex models.

	if we completely remove bias by adding complexity in the model, we have no bias but we are overfitting, means: the variance is too heigh. as we simple our model the bias is height, but variance is low.

	more complex the model: the low bias and heigh variance.

	algorithms with high bias will underfit the trainig data as well as perform poorly on the test/validation data.

	variance in the model is a bit hard to measure. you can look at how much you output cahnge is when you repeat the learning process under differen sample of the training data
	