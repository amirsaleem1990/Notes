Dimensionality reduction:
	Unsupervized learning / clustring technique, tries to reduce the number of features while losing as little information as possible.
	One simple technique for doing dimensionality reduction is principal componenet analysis (PCA)
		when we run PCA on data it can construct the n most infromative features. (these features are diffrent from existing data)


in Dicision Tree:
	restring the depth of the tree indirectly restricts the number of leaf nodes a tree can have. for example, having a maximum depth of three would allow at most 8 leaf nodes for that tree.

	overfitting can also be avoided by pruning decision trees. pruning can be done while building the trees, and this is called pre-pruning. Pruning can also be done after the decision trees have been built. this is called post-pruning. Pruning usually uses the cross-validation of validation scores to idendify which portions of the trees can be pruned. Pre pruning is also known as early stopping. and it's a faster method, but there are chances a could underfit the data as it gives just an estimate of when it should stop. post-pruning methods are not based on estimates, but rather actual values, and thedecision tree is pruned once it's completely built. 

	advantages of dicision tree:
		1- it is easiear to interpret then other models.
		2- can handle multi class classification
	diadvantage of dicition tree:
		they tend to be unstable. any little variation in the training data might result in totally diferent dicition tree. thus we use random forest. 

in Random forest:
	we train multiple dicision trees, eash with diffrest variables, (and bootstraping with replacement), then predict the most prediction. (eg: 50 trees, X ==> trees, 40 trees predict A, 5 predicts B, and 5 predicts C, we choose A as a prediction since it is most predicted)

	in the prediction phase, the observation goes into all trees, and most commont prediction choosen as prediciton for thtat observation.

	disadvantages:
		more dificult to interpret the Random forest then dicision tree. 

KNN:
	widely applied in many domains
	
	it is ofter useful to use it as a baseline test, no matter what your application is. 

	we need numerical representation for each variable (to campare distance, hence choose K closest)
	similaritly between two feature vectors can be measured by distance function.

	we don't need too larger K because that means listening to very distant neighbors. and lumping things together that are actually quite dissimilar. on the other hand, we don't want too smaller K, because then random meaningless local variations can have too much influence on our choice.

	(we know nearest neighbourhow graphically, but) computers can't glance and guess the way humans can, even in the two-dimensional case. (so we use distance functions to know distance. and distance functions require a numeric vectors)

	unlike most other classification methods, KNN is a type of lazy learning, Meaning that, there is no seperate training phase before classification. This does highlight one big downside ot thir algorithm, we have to keep the entire training data in memory. and as we get more examples it gets more and more expensive to computer which neighbors are nearest. for this reason, KNN usually works best on smaller data sets that don't have too many features. 

	There are two important decisions that must be made before building KNN model:
		1- value of K
			You can choose K somewhat arbitrarily, or could try it cross-validation to compe up with an optimal value for K.

			K should also be chosen such that there will be a clear majarity between classes.

			The choice of K is essential in building the KNN.
			in fact, k can be regarded as one of the most important factors influencing the KNN classification performance.

			K is smoothing parameter: 
				for any given problem a small value of K will lead to large variance in predictions. alternatively, setting K to a large vlaue, means you don't get a very customized answer, if k is the entire set of examples, you're going to always guess the majority class.

				K should be set to a value large enough to minimize the probability of misclassification, and small enough that it's actually a meaningful subset of the total number of examples.


		2- distance measure:

			(in KNN) do wee need always calculating all the  distances? NO, 
			(we have to scale our data before calculating the distance)
			a distance function maps two elements to some real number , the distance between them.

			There are 3 rules that must bold for valid distance measures. 
				
				1- only positive numbers
					distance(a,b) >= 0 # if a = b then distance(a,b) = 0
				2- order doesn't metter
					distance(a,b) = distance (b,a)
				3- the triangle inequality must hold (detouring from the staright line can only increase the distance)

			Q: There are many differny valid distance measures, what's the best? 
			Ans: as usual it depends on your problem

			Euclidean distance.

				Pythagorean theorem:
					sqrt(
						(x2- x1)^2 + (y2 - y1)^2
						)


					E(A,B):
						SUM = 0
						for j in range(1, m+1) # in python we start from 0
							SUM += (X(A,j) - X(B,J))^2
						sqrt(SUM)

				in Linear algebra:
					finding the magnitude of the diffrence vector between two points.
				
				it is genrelize to M dimensional space.


				it is also called (L2 NORM), L2 because we're squaring the values and norms is the technical term for the magnitude of a vector.

				Pehaps the most commonly used distance for machine learning algorithms.

				it's vary useful when our features are cotinuous, but there are some situations where Euclidean distance isn't quite right. As an example, if you're a taxi driver driving through the organized street blocks of a city then the straight line distance would
				force you to pass through buildings to get to your destination. So it's more accurate to consider what path you can take along the city blocks rather than straight line. In fact, we call it Manhattan distance.

			Manhattan distance:
				if we need to calculate the distance between two data points in a grid-like space. That grid-like space can be represented by what we call a Cartesian coordinate system. Cartesian coordinates describe locations by their distance along different axes. 

				For Manhattan distance, we take the differences along each of the axes and sum up those differences. This gives us the grid walking distances between two points, formally it's the sum of their absolute differences of their Cartesian coordinates. 

				distanceManhattan(A,B):
					SUM = 0
					for j in range(1,m+1) # in python we start from 0
						SUM += abs(  X(A,j) - X(B,j)  )
				Manhattan distance is better when we're dealing with high dimensional data or when you know, you can't go straight from point a to point b, then you might as well use the more appropriate measure. And because we're not squaring the difference like we did for Euclidean distance, we're not magnifying distances. This helps deal with outliers. 

				is also called L1 distance or L1 norm

				Euclidean and Manhattan distance measures are standard and useful measures, but they're only valid for continuous variables. If you have categorical or non continuous variables, the Hamming distance might be most appropriate. 

			Hamming distance only cares about differences, when the value's the same the distance is 0, if the values are at all different the distance is 1. It's easy to see that this works even for categorical features. The Hamming distance between two feature vectors is exactly the number of features that are different.

				distanceHamming(A,B):
					SUM = 0
					for j in range(1, m+1): # since in python we start counting from 0
						SUM += abs(  X(A,j) - X(B,j)  )

				if X(A,j) = X(B,j) ==> distanceHamming<j> = 0
				if X(A,j) != X(B,j) ==> distanceHamming<j> = 1



	with the power and simplicity, KNN also comes with downside of requiring mem ory for the training dataset and a fair amount of computation.



LOGISTIC REGRESSION:
	
	two steps:
		1- first linear regression
		2- logistic transfer function take linear function output and give some number(0,1)

	predict the probabilities that a data point belongs to a particuler class.

	it called <Logistic regression> because it's doing classification using a regression operation and a transfer function.
		transfer function:
			a function whose job it is to translate the output of one function into some other space. in this case the the trainsfer functions (which is logistic function here) takes the number reported by our regression model and transfer  it into a class label.
	the value it finds via the regression step can be thought of as the probability that a data poing belongs to a particuler class.

	The threshold can be adjusted based on the domin and whaterver bias you need for each class

	The logistic transfer function (or simply the logistic function) maps the entire real line and transform it into space 0 and 1.

	if we use square loss*** to evaluate the parameters of our logistic function(as in linear regression) that is not convex, since it is not convex we can't guarantee that we can get the botom of the hill it is actually the lowest possible point. we could get stuck in local minimum. so we get complementry loss, so that you DO get a convex loss landscape, these are called matching losses, if you have a maching loss for your particuler transfer function then the minimum point IS the global minimum which means you CAN find the best line. so once you solve the linear regression problem you can use the solution point and it will be the best solution after applying the logistic transfer as well. 
	logistic loss:
		squared error loss of the sigmoid function if the label is 0:
			y = -log( 1 / (1+e-f(x)))
		squared error loss of the sigmoid function if the label is 1
			y = -log( 1 - (1 / (1+e-f(x))))
	if you use this matching loss with a logistic transfer function you get a convex and smooth landscape. Now gradient decrease find your optimal model. 
	***	squared error loss of the sigmoid function if the label is 0:
			y = ( 1 / ( 1 + e-f(x))^2 )
		squared error loss of the sigmoid function if then label is 1:
			y = ( 1 - ( 1 / ( 1 + e-f(x))^2 ))
in Linear regression:

	Non Linear features and model complexity:
		
		to overcome underfitting we need to increase the complexity of the model. 

		Luckly, we can get complexity in Linear function through features enginearing.
		 	polynomial, square root, log
		
		polynomial:
		 	multiplies things together.
		 	the order(degree) of polynomial tell you how many times you multiply then.
		 	order2 polynomial = squared = X * X
		
		we create non Linear functions by adding or changing features that are non Linear functions of the orignal features.

		non Linear features does increase the complexity of the model which has sight affects. thre are some issuse we have thing about when we use non Linear features to add complexity to the model. 
			(much hight degree of polynomial feature causes overfitting, so be carefull)








	Linear functions have handy features like convexity and differentiability.  
	Goal: 
		find parameters (weights) that minimize the loss function.

	least squares AKA L2 loss function.

		this loss is useful because it is gives a shape that have a unique point for the minimum panalty(SSE).
		smooth and convex. which means it is going to have unique minimum point, we don't know what that minimum is, but using calcules we can find id.

		Q: is this always the panalty you want to use?
		A: NO.

		Q: we could just take the absolute value of diffrence (y^i - yi) to get the actual magnitude, why not do that?
		A: we do, and that is totally walid loss function, is is called <L1 loss> or <least absolute error. when we use gradient descent bot L1 and L2 are convex, but L1 has sharp corver <V shape> at minimum value (thus this is not smooth, thus it is not differentiable, we can't take the darivite of the L1 loss to find which parameter gives us the minimum panalty like we did with L2 loss function.> instead of smmoth curve like L2 <U shape>. 


	gradient descent:
		very commonly use in machine learning

		in gradient descent we rely on the property that our graph (loss vs slop) is smooth and continuous, so that we can take tha derivative, and convex so thre is no more than one minimum value.

		an iterative method, we start fron any point then make small adjustments in our perameters(in regression the coefficient/s) to bring us closer to an approximate solution. 
		we choose point where darivite is 0.
		require smooth function (so we don't use L1 loss function)

		We randomly pick a starting position and imagine placing a ball there. For minimizing loss, this amounts to assigning random parameters to our model. This is simply an initial guess, we have to start somewhere before we can improve it. Say our random initialization is peer. We hinted that the ball will roll down the pill somehow, but we can exactly calculate in what direction the ball will roll using the derivative or the slope of the tangent to the graph at this point. We'll use this derivative to take small steps towards the minimum of our graph. We have this mathematical notation for how to take a step or in other words, how we can tweak that parameter w. We update it by taking the current estimate then subtracting alpha times the derivative with respect to w of the loss function of the current estimate. Let's go through that bit by bit to see what's going on. Of course, we start at time zero, so w0 is our first guess for our model parameter. Now we want the ball to roll down the hill, but we need to know which direction is down. Hurray for calculus, that's the gradient of the loss function, the derivative with respect to w. So that's what direction to step, but how far do we adjust w? We need to know how large a step to take. There's a parameter for this known as the learning rate or step size. Here in our equation, we've represented this with the Greek letter alpha. It's your choice what learning rate to use, but it's some number greater than zero or you wouldn't learn anything and smaller than one or you'll over correct. Multiplying the learning rate by the gradient determines how much we should change the current position on the horizontal axis. Remember, this is the value for our models parameter w. Notice that the steeper the gradient, the bigger the change and vice versa. When the gradient is less steep, the learning rate multiplied by the gradient will be smaller. So the amount we update our position by will be smaller. This is true even if we have a rather large learning rate. What is a good learning rate to use then? It turns out this is a hard question to answer. A good learning rate is one that's not too big because you'll end up stepping past local minima and you never settle on a good parameter value. But can't be too small or it will take forever to converge, to settle down near the best value. So we want to have alpha that's as big as possible so that it will learn quickly but not one that's too big and diverges. Some techniques have decreasing alphas so that you can start big but settled right down, and depending on the rate of decrease, you can actually guarantee convergence. But it's usually simplest to sacrifice optimal convergence for our convenience and use some small alpha. The best alpha depends entirely on your particular problem. When in doubt, try 0.1, if that seems unstable, try 0.01 or start with a default set by your particular library. Enough about alpha. Let's look at the key operation in this equation, subtraction. We're taking our current position and subtracting the learning rate multiplied by the gradient. This gives us our new value which we really want to be closer to the optimal value. So why subtraction? Notice that at time t, we're here on our graph. In our 2D example, we can see that in order to get closer to the minimum, we want to step to the right. Stepping to the right is equivalent to increasing w along the horizontal axis. Notice that at this position the gradient is negative. The learning rate is always a positive number so subtracting a negative number from our current position moves us to the right. Similarly, if we'd been located here at time t, we'd want to move left to get closer to the minimum. Our gradient at this point is positive and so subtracting it from our current position will make w smaller moving us to the left. To summarize, because we've made sure our loss graph is smooth and convex the derivative or gradient always exists and always points away from the minimum point. In order to get closer to the minimum, we move against the gradient. This is why we have a minus sign in our update equation and also why we call it gradient descent. The process is iterative, so after subtracting the gradient multiplied by our learning rate, we have a new position for our ball. We calculate the gradient at this new point, multiply it by our learning rate and subtract that from the current position. Repeat this process over and over until we're not changing our position very much because the gradient has gotten so small. This lets us converge on or close to the minima. What does convergence mean? Remember, that local minimums happen when the derivative is equal to zero. As we step closer and closer to the minimum, the derivative will get closer and closer to zero as long as we're not overstepping. When we arrive at the minimum, the change will be zero and so our parameter value won't change anymore. We converge to the position where the minimum occurs.


	Across all loss functions, there's one crucial property that each must
	possess if you want to guarantee you're finding the global minimum. This
	property is convexity. The function is called convex if, for any two points
	on the graph of the function, the line segment connecting those two points
	lies above or on the graph. If you think of functions as describing some
	surface, you have to be able to attach the ends of a string anywhere along
	the surface and pull that string flat. If there's some way to attach the
	string where you can't make it absolutely flat without intersecting the
	surface, then it's not a convex surface. Why is convexity important?
	Convexity guarantees that our function has no more than one local minimum
	value.

		When our graph is not convex, we don't know for sure we've found the absolute smallest minimum value unless we explore the entire space. Whereas, no matter where we drop that ball on a convex graph, we'll reach the same final point at the only local minimum. That's the key, convex functions have at most one minimum value. So regardless of whether or not you use derivatives, if your function is differentiable or some other iterative numerical solver, if your function is not differentiable, when your function is convex, you're guaranteed to find the optimal solution in that hypothesis space. So one local minimum means we're able to find the smallest penalty value for our loss function. And if our loss function is differentiable as well as convex, this means we may be able to find an optimal function by setting the derivative of our loss function to zero, as we did in the case of L2 loss. 


bias variance tradeoff:
	
	complexity is not always good. we need our model to be complex enough to capture the relevent patterns in the data but too complex menas heigh variance and it is not ganralized well. 

	as we complex our model; in some range of complexity the bias and variance both decrease, after that poing the bias still decrease and variance increase (means: we begin overfitting), we have to choose optimal model.

	(very low bias: overfitting)

	one way to ovoid overfitting is to refuse to allow complex models.

	if we completely remove bias by adding complexity in the model, we have no bias but we are overfitting, means: the variance is too heigh. as we simple our model the bias is height, but variance is low.

	more complex the model: the low bias and heigh variance.

	algorithms with high bias will underfit the trainig data as well as perform poorly on the test/validation data.

	variance in the model is a bit hard to measure. you can look at how much you output cahnge is when you repeat the learning process under differen sample of the training data. you can also take advantage of relationship between complexity and variance, and use tricks incurage simple models.

	Reguralization A systamatic way of macking of tradeoff between bias and variance.

	learning curves can also be used dignose your model performance. 



Reguralization:
 
	A systamatic way of macking of tradeoff between bias and variance.
	A way to avoid too complexity in the model.
	
	when a learning algorithm has an objective function that only penalizes mistakes as with the loss functions we've already described, there's no reason for it to choose a simple model. So as usual, we need a mathematically precise way to measure simplicity if we're going to ask the computer to consider it. We want to modify our objective function with some criteria that rewards simplicity as well as accuracy. Just like we converted best line to least bad by penalizing mistakes, we can convert most simple to least complex and add a penalty for complexity. This is the idea behind regularizers. A regularizer is an extra mathematical term added to the objective function which penalizes complexity. Now the objective you want the learning algorithm to minimize is a combination of the penalty for making mistakes as well as the penalty for using a complex model.

	As your model gets more and more complex and more and more fitted to the training data by using higher and higher degree polynomial features, you can observe that the weights are getting larger and larger. The magnitude of weights looks like a good proxy to measure complexity and that's often what we use. We use a regularization term to penalize extreme values for our parameters. How do we measure extreme values? We sum them up. As usual, since we care about the magnitude and would prefer to keep things differentiable, the L2 norm is our friend. Remember that the L2 norm squared is the sum of the components squared. It's the magnitude of the weight vector. Remember also that the L2 norm penalty makes extreme values cause even more lost than an L1 norm penalty would because it's squares those values. But we want a little more control over this term because we might want to explore how much we should penalize the weight values. We do that by turning to another common trick in machine learning, introduce a coefficient, in this case lambda. Lambda acts as an adjustment knob to control the relative strength of carrying about simplicity compared to carrying about loss. With lambda equals zero, your objective function simply considers loss like before.

	As you turn this knob up to increase lambda, you penalize the learning algorithm more and more for complexity via the size of the weights. This means, your higher degree model gets simpler and simpler through having less and less extreme values which has a slight cost in terms of increasing average loss on the training data, but there is a benefit. You reduce the variance in your model generalization. This usually improves performance on test data at least up to a certain point. Lambda is another one of those smoothing parameters. It lets you control the trade-off between complexity and generalization between variance and bias in your model creation. However, this is not the only thing we get with regularizers. In some cases you can get interesting benefits by using different kinds of regularizers. For example, using an L1 norm of the weight vector as your regularizer term causes many models to try to find a sparse solution. This means the algorithm will try to find a model whereas many of the parameters as possible are equal to exactly zero instead of just being small. This means you can discard many of the features and use a smaller subset of features in your model.

	Regularizers also provide other benefits when combined with different kinds of specific losses. For example, when you combine L2 regularizers with SVM classifiers, we get the ability to use kernels. Kernels let you classify data points in a complex and sometimes infinite-dimensional space of nonlinear features.

	The concept of regularization can be applied almost anywhere

	Linear regression with an L2 regularizer is called ridge regression.
	Linear regression with an L1 regularizer is also called Lasso regression.



 Neural network:
 	neuron is basic component of Neural network.
 	each neuron take some numbers of input and give a (single) number as output.
 	typically each neuron actualy envolve two saparate simple functions. 
 		1- a linear function.
 			because linear functions are differentiable and convex, it realy easy to calculate, out hypothesis space is too small when we only look at linear function. 
 		2- then a non linear transfermaton(AKA activation function).

 		multiplies its weights against the input values and returns the sum. then that value is passed to activation function, and outcome is some other number, this output is the final output from this neuron.

 	Neural network learning these weights using gradient decrease.

 	output of on layer is a input of next layer.
 	input layer , following by hidden layer/s , and the output layer(which is actualy a predictions)
 	in between the input layer and output layer can be ANY number of hidden layers. 
 	the layers can be loop back from themselves, but dafault is the feed forward Neural network; that feeds each hidden layer with the output of the layer before.
 	one we have multiplied hidden layers the Neural network is called deep Neural network. and the process of learning is Deep learning. 

 	sigmoid function = logistic regression
 		take inputs from privious layer, run linear regression, the output of linear regression go into logistic function, and give some output(0,1)

 	with then Neural network we don't need to optimize each neuron saparately, 	it isn't clear how we could optimize each neuron saparately. we know what we want the entire network to do. but we DO know we need each layer to output values that ware makes the work of next layer easier. 

 	Neural nets can't be used for everything, we can't explain the results. 

 	the huge hypothesis space explored by neuron networks created variance, this is why the lots of data is incredibly important.


SVM (suport vecter machine):

	the combenation of the loss and the transfer function has to be a convex surgate. 
	lossMisclassification(y^i, yi) =  1 if yi*y^i=-1, and 0 if yi*y^i=+1
	+1 * -1 = -1
	-1 * +1 = -1
	+1 * +1 = +1
	-1 * -1 = +1
	with this missclassification loss  we treat every missclassification equqly bad, but may be some missclassifications are worse then others. remember we are trying to decide how bad a linear saperation of the two classes is. may be we can impose stronger panalties for samples found deeper in taratory of the other lass. in fact we wouldn't mind the margen for errors. so even correctly classified samples  but are really close to the line almost missclassified could increase the panalty. 

	hinge loss function:

		this function solves some of the problem created by other loss functions, and can be used to turn the power of regression towards classification. 
	
		we compute the panalty:
			z^i = xiT w
			and then use sign function to determine which side of the line taht particular example falls on.
			y^i = sign(z^i)		

			if z^1 = 0: 
				then that example x is exactly on the decition boundry.

			so in the linear part of our function the points further from the line will be mapped to increasingly positive or increasingly nagative numbers, depending on which side of the line they are on.  we can use that number (z^) as a notion for signed distance from the line. we can use it to create a panalty that magnified by distance from the linear sapareter, if we take this notion of signed distance (Z^) and multiply by the correct lable (y) we get that number that posative if the sample falls in correct side of the line. or nagative if the point falls in wrong side of the line. and it is really positive a very large number if it is realy far on the correct side of the line.  and it is realy small when it is quit close to dicision boundry.	
			 

			we want to penalizes nearly wrong classification create some margen fro error arround the dicision boundry. we typically give ourselves margen of 1, we want to penalizes plasing the line less then 1 unit away from a point evan if it is correctly classified. for this we just need to subtract our y*z^ from 1 <1-y*z^>, this value will be posative for any point that encroaches on that margin, or is incorrectly classified by our line. 
				this way not only incorrectly classified points get panalty but correctly classified points that are in that margin space around the dicision line will also recive small panalties. 

				how ever if use this <1-y*z^) as our loss  the correctly classified points will get a nagative panalty which is wrong. we can fix it by filtring losses less then 0, this bring us the final formula for hinge loss:
					max{0, 1-y*z^}  

				we sum this panalty over all training data, and that is our total loss. 
				this hinge loss in convex (oposite to misclassification loss). 
				we are ready to use the hinge loss to choose good linear saparater for our classification task(SVM). 

	SVM a powerfull techniques for classification. 
	
	the best line maximizes the distance between itself and the closest data points from both classes. This distance is known as the margin. Since Support Vector Machines are trying to maximize the distance, SVMs are also called large margin classifiers, which is the line that creates as much space as possible between the closest points to the decision boundary, which are at least two, one from each class. More precisely, in the case where the data is linearly separable and we don't allow any misclassification, it's the line that's the same distance from the closest points from both classes. These points which are closest to the best line are called support vectors. Thus, the machinery that uses such a line is called the Support Vector Machine or SVM. SVMs are nothing other than a linear model with a specific separating line. That specific line is the line that causes the widest minimum margin. So what optimization process is happening while that SVM is being learned? You won't be surprised to hear that hinge loss takes the center stage. Just like linear regression finds the weights that minimize the loss and can use regularizers to control complexity, a Support Vector Machine finds the weights that minimize the aggregate hinge loss with an L2 norm as weight regularizer. However, the L2 regularizer is playing a very important role here. It's the L2 regularizer that's ensuring the minimum margins are being maximized.

	we can show with math that the minimum margin is actually two over the L2 norm squared of w

	So minimizing the L2 norm squared regularizer causes maximum minimum margin. The constant C in the equation shown is the only new piece of this puzzle. Just like before, we have an arg min over the weight vector w and b. The inner term calculates the penalty over all the mis-classified values. Here, it's shown in the hinge loss formula as the max of 0 and 1 minus y times z hat. That's multiplied by a constant c, and then the L2 norm squared of the weight vector is added. If your data is linearly separable and you use a very high value for c, minimizing the SVM objective function finds exactly the line that gives you the widest minimum margin possible. This is due to the interplay between three things. One, the triangle you see on the positive side of the yz hat axis, the fact that you're penalizing anything that has a margin less than one. Two, the fact that you can change the magnitude of the w vector without affecting the direction of the line. Three, that you're pushing w to be as small as possible with the regularizer added and if the data is not linearly separable, its penalizing misclassifications more and more as they show up deeper and deeper on the wrong side of our decision boundary. C is a smoothing parameter that controls the trade-off between model complexity and accuracy. When c goes to infinity, the SVM will be called a hard margin SVM. Other c's make soft margin SVMs. Hard margin SVMs work only on linearly separable data and will force all hinge losses to be exactly zero. In soft margin SVMs, the smaller the c used or the softer you get, the more the algorithm will prefer to put points on the wrong side of the separation line if it keeps the norm of the weight vector down, causing a bigger margin among the points that are on the correct side, but also likely increasing the generalization power, at least up to some point. So what does that make c? Insert quiz here. Optimizing the SVM objective function can actually happen several different ways, and the good news is you don't really have to worry about how it happens. It can use a gradient descent method or quadratic programming, which is an iterative method. So when are SVMs a good choice of learning algorithm? Well, because of the kernel trick which we'll discuss next, they actually handle nonlinear relationships rather well. To be fair, so do your neural networks but because SVMs are based on convex optimization, you don't need as much data to have confidence you're finding the best model within the hypothesis space. When you do have lots of data in the millions of examples, SVMs can be a little slow to train. But predicting with an SVM model is quick, especially compared to Kernels with large datasets or deep neural networks. They do require homogenous appropriately scaled features. If you have mixtures of features, such as categorical and numeric, you probably want to consider decision trees or random forests.

	you can use SVMs for regression as well

	
Regression assesment:
	MAE (mean absolute error)
	MSE (mean squared error):
		if we want to convert the units back to orignal form; we can take the square root of MSE, and this known as RMSE (root mean squared error), we might want to report RMSE rather then MSE just so that we have some intuition about what that average error actually means. but is it unnecesery to do square root operetion if you mostly looking at comperison.

		MSE is most common error measure.

		There is one side effect of squaring the errors, squaring big numbers even bigger, so while MAE penalizises each error equalantly, MSE in increasingly sensitive as the errors get bigger. in other words: by MSE the model batter of macking several small mistakes then one big one. The practical consequences of this are that MSE sacrifices perfect fit on a few data points to compensate for big mistakes. In other words, one outlier in the test data makes our model look worse under MSE than it would under MAE. Sometimes this is what you want. If you have confidence that your outliers represent actual phenomenon that needs to be modeled you should be emphasizing large mistakes. There's another factor we might want to consider in our assessment of our models. Remember how different kinds of mistakes can matter more than others depending on your problem. Maybe there's some reason that underestimating values is not so bad as overestimating them, which can be especially true when the model will be used for classification, or maybe there's a certain range that's really important to get right. In that case you want a weighted error function. A weighted error function lets you assign different weightings to different kinds of error, for example, penalize overestimating values twice as heavily as underestimating. This is more common in classification error measures. 

	mean squared log error adds a log function to the calculation of the difference. This results in underestimates being penalized more than overestimates, and more importantly, evening out the effect of mistakes, not being as sensitive to outliers. It's useful when you care more about relative mistakes than the absolute magnitude of your mistakes. 
	
	R^2:
		1 - SSE / SST 
		R^2 looks at the variation or noisiness of the model labels. In particular, it modifies the same square error measure as MSE, but divides by the noisiness in the labels themselves. This means R^2 is normalized, and it lies between 0 and 1, with 1 being the best score. R^2 is usually used for interpretation without knowing the scale of the data and it explains how well the variation in the data itself explains the variation in the predicted values.


classification assessment:
	type2 error = FN(false nagative)
	type1 error = FP(false positive)


	confusion mutrix:
		columns : actual claass
		rows    : predicted class

	Accuracy
	Precision: 
		TP / (TP + FP)
		measures how mush can trust the accuracy of your positive examples. in other words: of all the things we called positive, what percentage were correct? Precision of 1  means we never call a transaction fraudulent inappropriately. When our QuAM called a transaction fraudulent, you can be confident it's correct. 
	Recall:
		TP / (TP + FN)
		Recall measures how much you can trust the coverage of our positive labels. In other words, what percentage of all the positive instances did we catch? Recall of 1 means we didn't miss anything. 

	If you know what's most important not to call something category X when it isn't, then you should be evaluating your models using precision as well as accuracy. If you know what's most important not to miss something belonging to category X, then recall is the most important. But what if you want to balance between those two? Well, use the F-measure gives us the ability to do that.

	F-measure:
		When the F Score is 1, it means we have perfect precision and recall. That also means we have perfect accuracy. So we do already have a measure for that. The usefulness of S QuAM comes into play when we don't have perfect accuracy. It reports the average, specifically the harmonic mean, but you don't need to worry about that, of precision and recall. The default way to use F score is specifically F1 score and that puts equal weight on precision and recall. The weighted F beta score let's use smooth between precision and recall with beta less than 1 putting more weight on precision and beta greater than 1 weighting recall more heavily. There's another case where plain accuracy may not be good enough when you have imbalanced classes. Then misclassifying those rare instances counts just as much as misclassifying the other. But if there's only a few examples of the rare case, overall the significance of those mistakes is way less than the significance of the other. In the worst case, the best hypothesis will be the one that completely ignores the rare cases and just predicts the majority class. This may be fine in some circumstances, but not if the QuAM building is meant to detect those rare cases. When you want to know how well classifiers performing specific to a class label, you can use class wise accuracy. Evaluation measures are specific to the domain. So you should think about how your model's going to be used to determine what the most useful measure is. The evaluation measure you use on the test data is what you use to determine how well your model is going to actually perform in the real world.
 
		F1:
			2 * ((precision * recall) / (precision + recall))

learning curves: 
	find reight complexity for model, and avoid overfitting.

	two key	 factor effect the generalization ability of model:
		1- model complexity
			as model complexity increases; decrease training AND test errors. after cirtien point the model begin overfitting.

		2- dataset size
			if you have more data for training; the ability of your model to generalize almost always become batter.
				
			(givin the model complexityi is fixed) as increases our data size the training error increases and test error decrease.
				training error increases because the variance of the training data increased where the model complexity will fixed.

				test error decrease because the model can now generalize batter.

			If we were to plot the learning curves of the training test data for different training data set sizes for fixed model complexity; The model will have low training error when we use a small training data set. However test error will be really high because it hasn't learned much about the variations in the data distribution to be able to predict for out-of-sample instances. In other words, the data points the model hasn't seen before. However, when more training data instances are used, training error increases. This is because the variance in training data has increased while the model complexity was fixed. However, test error goes down because the model can now generalize better.Another type of learning curve that specifically measures the performance of classification models is the: 

	ROC (or Receiver Operating Characteristics curve)
		For binary classification, when you have a choice of threshold as in the case of transfer functions over probabilities, the ROC curve lets you see the trade-off between precision and recall. It illustrates the capability of the classifier to distinguish between two classes as the class discrimination threshold varies. This is achieved by plotting true positive rate against false positive rate.remembter: recall is the true positive rate and false positive rate can be considered false alarms. A point in the ROC curve represents the classification model with a specific threshold setting determining the class and the ROC curve represents the collection of such points. The best possible classification model with the right threshold setting would give a point in the upper left corner or the coordinates 01. This essentially means no false negatives and no false positives. On the other hand, a random guest such as a fair coin flip would give a point along a diagonal line which goes from the bottom left to the top right. Any point which falls above this diagonal line represents a classification better than random guessing. While the ones which fall below represent classifications worse than random guessing. AUC or Area Under the Curve of the ROC curve represents the probability a classification model will rank a randomly chosen positive instance higher than a randomly chosen negative one. In other words, get it wrong. An AUC values varies between zero and one, where one means the classifier has excellent performance in separating between two classes and zero means the worst possible. 0.5 means the model doesn't actually separate the classes. Contrasting training with test error lets you see the cost of complexity and know when decrease in training error is actually hurting you. For classification, you can use the AUC ROC values as a more nuanced tool to see the trade-off between precision and recall.
	
I.I.D:
	independent identical distributed data.

	None of the observations (spacially None of the lables) depends on each other in a probablistic sence. this whill  mean you can shuffle the order and your label for one example doesn't change the probablity of the correct label for the next.	

	Not I.I.D:
		eg: time series data.

		randomely split NOT IID data into train and test causing two problems:
			1- you just lost the ordering; which is importand here.
			2- you don't actually have a cleaned test set. 
		
		there are 2 ways to deal with this:
			1- get rid of dependencies.
				you can't do it for any odd data. it depends on the data.
			2- not split your data randomly.

splitting data into train and test:
	(when we split our data into train and test, we make sure same distribution of outcome variable in bot tran and test set. eg: we have 100 observations, and label has 2 classes, 0:80%, 1:20%, we make sure after the split that the both train["target"] and test["target"] have the  0:80%, 1:20%)  

	we NEVER use test data to improve our model. (eg: it is wrong to split data into train and test, then train your model; then test on test data; after testing you go to tune your model. this is wrong. the test data should use ONE TIME ONLY, after you all tuning using validation set.). so the right way: 
		1- split into train, validation, and test chunks. 
		2- train on train set
		3- validate on validation set
		4- tune your model
		5- repeat 2,3,4 until the model is final.
		6- test on test set. and the results SHOULD NOT be used to tune the model.
		7- deploy the model.


CV (cross validation):

	Cross validation is mainly used when the dataset is relatively small. results of cross validation will be a good approximation of the result we wol'd have gotten if have used the full data set. This can help alleviate the problem with small datasets able to represent the true distribution at least to some degree. 

	we use CV to tune for hyper	parameters in general. 

	when you have lot of data make a validation set; the same you make a test set. (but) when you don't have data to spare use the appropiate cross-validation techniques. 


hyper parameters tuning:
	finding a best hyper parameters set is called Metalearning.

	(in dicision Tree:
			max depth
			min samples split (dont split further if node have less then this vlaue observations(amir)) 
			min samples leaf  (dont split further if one of the resulting nodes have less then thin value observations(amir))
			criteria for quality of splist (gini index, entropy)
			cp
			threshold (in classification)
			num trees (random forest)
			replace   (random forest)
			variables qty (random forest)
			sample.fraction (random forest)
			minimum_improvement (if at any point; spliting the space <= this value then don't split it)
			pruning (after or before)
			MSE or MAE
			weights
		in logistic regression:
			threshold
			weights
		in linear regression:
			alpha (if Reguralizers are used)
		in SVM:
			c
			weights
			distance function
		in KNN:
			k
			distance function
			weights
		in clustring:
			K (in k-mean)
			distance function)

	paremters of model that we have to put in the model. eg: max_depth and min_node in dicision trees, and threshold in logistic regression, and alpha in regularizers, and learning rate in gradient descent, and so on...

	on the other hand <model parameters> that the model is care about, eg: weights in the Neural network nodes and the split point in the dicision tree.

	thre are couple of techniques to identify best combination of hypter parameters:

		1- Grid search 
			computationally expansive
			systematic aproach,  you are give the list os pre determined combinations. the model trained on each combination, and you pick bech of them.
		
		2- random search 
			you provide a range of values for each hyper parameter. the system select random values within these ranges, and try diffrent combinations.

			it is usually finds optimal parameters or close to the optimal parameters when the data is small and the number of features is relatively few. 

			Researches shown that the random search actually does a batter job of finding optimal parameters then grid search. 
		
			instead of list of which value explicitily to try for each hyper paramter(as in Grid search), we have two options:
				1- list
					the functions will randomly sample value without replacement from the list. 
				2- distribution  
					you have also to supply the sampling method. 

