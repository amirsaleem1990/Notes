CNN (convolution neural networks) for vision
RNN (recurrent 	 neural networks) and LSTM(Long short-term memory) for sequance data (temporal or NLP)

RELU:
	rectified linear unit
	rectified: if the prediction from linear function is nagative then the prediction is chaneged to 0, so using RELU we get all predictions >= 0

audo is represented one dimontional time series

text, audio, and images are unstructred data. 

much harder for computers to make scence of unstructred data compared to structered data.
  

agar data kafi zyada na ho to ML use karty hen, jahan par koi improvement lany k lye zaroori h k feature enginearing aap khud karen. or agar aap k pas data bohot zyada h to aap us ko DL model me fit kar sakty hen, jahan aap ko feature enginearing ki zaroorat nahi h.

DL k lye zaroori cheezen:
	very large data
	high level hardware (eg: ram and GPU)

sigmoid = logistic regression
relu    = linear regression (except that nagative predictions replaced with 0)

relu Vs sigmoid (as activation function):
	in relu the gradian descent run much faster compaired to sigmoid
	in sigmoid there is  nearly flat line in bot tails (upper and lower), so gradiant is nearly 0, so learning become very slow. 

	on the other hand the gradiant is equal to 1 for all positve values of input, so gradiant descent is much faster  

sparce matrix:
	matrix with lot of zeros

Logistic regression:
	we interpret the output of logistic regression as follows:
		probability of y=1 given this observation
	